---
title: "\U0001F3B2 Samples, Populations, Statistics and Inference"
date: 25/Nov/2022
date-modified: "`r Sys.Date()`"
order: 20
abstract: "How much ~~Land~~ Data does a Man need?"
image: preview.jpg
categories:
- Sampling
- Central Limit Theorem
- Standard Error
- Confidence Intervals
bibliography: 
  - grateful-refs.bib
citation: true
filters: 
  - shinylive
editor: 
  markdown: 
    wrap: 72
---

## {{< fa folder-open >}} Slides and Tutorials

|  |  |  |  |
|--------------------------------------------------------------------|---------|---------|--------------------------------------------------------|
| <a href="./files/sampling-tutorial.qmd"><i class="fa-brands fa-r-project"></i> R Tutorial</a>   |  |  | <a href="./files/data/qdd-data.zip"> <i class="fa-solid fa-database"></i> Datasets</a> |

:::: pa4
::: {.athelas .ml0 .mt0 .pl4 .black-90 .bl .bw2 .b--blue}
["When Alexander the Great visited Diogenes and asked whether he could
do anything for the famed teacher, Diogenes replied: "Only stand out of
my light." Perhaps some day we shall know how to heighten creativity.
Until then, one of the best things we can do for creative men and women
is to stand out of their light."]{.f5 .f4-m .f3-l .lh-copy .measure
.mt0}

[--- John W. Gardner, author and educator (8 Oct 1912-2002)]{.f6 .ttu
.tracked .fs-normal}
:::
::::

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
library(tidyverse) # Data Processing in R
library(mosaic) # Our workhorse for stats, sampling
library(skimr) # Good to Examine data
library(ggformula) # Formula interface for graphs

# load the NHANES data library
library(NHANES)
library(infer)

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(grateful)
library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(smovie) # Stat Movies with code
library(visualize) # Plot Densities, Histograms and Probabilities as areas under the curve
library(shinylive)
```

#### Plot Theme

```{r}
#| label: Plot Sizing and theming
#| code-fold: true
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


```{r}
#| echo: FALSE
#| eval: false
#| fig.align: 'center'
#| fig.alt: "Photo by Anirudh on unsplash"
knitr::include_graphics("preview.jpg")

```

## {{< iconify clarity group-solid >}} What is a Population?

A *population* is a collection of individuals or observations we are
interested in. This is also commonly denoted as a study population in science research, or a *target audience* in design. We
mathematically denote the population's size using upper-case `N`.

A *population parameter* is some numerical summary about the population
that is unknown but you wish you knew. For example, when this quantity
is a mean like *the mean height of all Bangaloreans*, the population
parameter of interest is the *population mean* $\mu$.

A *census* is an exhaustive enumeration or counting of all N individuals
in the population. We do this in order to compute the population
parameter's value exactly. Of note is that as the number N of
individuals in our population increases, conducting a census gets more
expensive (in terms of time, energy, and money).

::: callout-important
## {{< iconify carbon parameter >}} Parameters

Populations *P*arameters are usually indicated by Greek Letters.
:::

## {{< iconify game-icons card-pickup >}} What is a Sample?

Sampling is the act of collecting a small subset from the population,
which we generally do when we can't perform a **census**. We
mathematically denote the sample size using lower case `n`, as opposed
to upper case `N` which denotes the population's size. Typically the
sample size `n` is much smaller than the population size `N`. Thus
sampling is a much cheaper alternative than performing a census.

A **sample statistic**, also known as a *point estimate*, is a summary
statistic like a `mean` or `standard deviation` that is computed from a
sample.

::: callout-note
## Why do we sample?

Because we cannot conduct a census ( not always ) --- and **sometimes we
won't even know how big the population is** --- we take samples. And we
*still* want to do useful work for/with the population, after estimating
its parameters, *an act of generalizing* from sample to population. So
the question is, **can we estimate useful parameters of the population,
using just samples? Can point estimates serve as useful guides to
population parameters?**

[This act of generalizing from sample to population is at the heart of
**statistical inference**.]{style="background-color: yellow;"}
:::

::: callout-important
## An Alliterative Mnemonic

NOTE: there is an
[*alliterative*](https://www.grammarly.com/blog/alliteration/)
[*mnemonic*](https://www.merriam-webster.com/dictionary/mnemonic) here:
**S**amples have **S**tatistics; **P**opulations have **P**arameters.
:::

## {{< iconify fluent-mdl2 test-parameter >}} Population Parameters and Sample Statistics

|                    | Population Parameter | Sample Statistic |
|--------------------|----------------------|------------------|
| Mean               | $\mu$                | $\bar{x}$        |
| Standard Deviation | $\sigma$             | s                |
| Proportion         | p                    | $\hat{p}$        |
| Correlation        | $\rho$               | r                |
| Slope (Regression) | $\beta_1$            | $b_1$            |

: Parameters and Statistics

::: callout-note
## Question

Q.1. What is the mean commute time for workers in a particular city?

A.1. The *parameter* is the mean commute time $\mu$ for a *population*
containing all workers who work in the city. We *estimate* it using
$\bar{x}$, the mean of the random sample of people who work in the city.
:::

::: callout-note
## Question

Q.2. What is the correlation between the size of dinner bills and the
size of tips at a restaurant?

A.2. The *parameter is* $\rho$ , the correlation between bill amount and
tip size for a *population* of all dinner bills at that restaurant. We
estimate it using r, the correlation from a random sample of dinner
bills.
:::

::: callout-note
## Question

Q.3. How much difference is there in the proportion of 30 to 39-year-old
residents who have only a cell phone (no land line phone) compared to 50
to 59-year-olds in the country?

A.3. The *population* is all citizens of the country, and the parameter
is $p_1 - p_2$, the difference in proportion of 30 to 39-year-old
residents who have only a cell phone ($p_1$) and the proportion with the
same property among all 50 to 59-year olds ($p_2$). We estimate it using
($\hat{p_1} - \hat{p_2}$), the difference in sample proportions computed
from random samples taken from each group.
:::

Sample statistics vary and in the following we will estimate this
uncertainty and decide how reliable they might be as estimates of
population parameters.

## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Sampling the NHANES dataset

We will first execute some samples from a known dataset. We load up the
NHANES dataset and glimpse it.

```{r}
data("NHANES")
glimpse(NHANES)

```

Let us create a NHANES (sub)-dataset without duplicated IDs and only
adults and select the `Height` variable:

```{r}

NHANES_adult<-  
  NHANES %>%
  distinct(ID, .keep_all = TRUE) %>% 
  filter(Age >= 18) %>% 
  select(Height) %>% drop_na(Height)
NHANES_adult

```

### {{< iconify mdi head-thinking-outline >}} {{< iconify clarity group-solid >}} An "Assumed" Population

::: callout-important
## An "Assumed" Population

Normally, we very rarely have access to a **population**. All we can do
is sample it. However, for now, and in order to build up our intuition,
we will **treat** this single-variable dataset as our **Population**. So this is our *population*, with appropriate *population parameters* such as `pop_mean`, and `pop_sd`.
:::

Let us calculate the **population parameters** for the `Height` data
from our "assumed" population:

```{r}
# NHANES_adult is assumed population
pop_mean <- mosaic::mean(~ Height, data = NHANES_adult)

pop_sd <- mosaic::sd(~ Height, data = NHANES_adult)

pop_mean
pop_sd

```

### {{< iconify game-icons card-pickup >}} Sampling

Now, we will sample **ONCE** from the NHANES `Height` variable. Let us
take a sample of `sample size` 50. We will compare **sample statistics**
with **population parameters** on the basis of this ONE sample of 50:

```{r}
#| layout: [[20,20,60]]
#| warning: false

# Set graph theme
theme_set(new = theme_custom())
#

sample_50 <- mosaic::sample(NHANES_adult, size = 50) %>% 
  select(Height)
sample_50

sample_mean_50 <- mean(~ Height, data = sample_50)
sample_mean_50

# Plotting the histogram of this sample
sample_50 %>% 
  gf_histogram(~ Height, bins = 10) %>% 
  
  gf_vline(xintercept = ~ sample_mean_50, 
           color = "purple") %>% 
  
  gf_vline(xintercept = ~ pop_mean, 
           colour = "black") %>% 
  
  gf_label(7 ~ (pop_mean + 8), 
          label = "Population Mean", 
          color = "black") %>% 
  
  gf_label(7 ~ (sample_mean_50 - 8), 
          label = "Sample Mean", color = "purple") %>% 
  
  gf_labs(title = "Distribution and Mean of a Single Sample",
          subtitle = "Sample Size = 50")


```

### {{< iconify ic baseline-loop >}} {{< iconify game-icons card-pickup >}} {{< iconify icon-park-outline average >}} Repeated Samples and Sample Means

OK, so the `sample_mean_50` is not too far from the
`pop_mean`. Is this always true? 

Let us check: we will create 500 repeated samples, each of size 50. And calculate their mean as the *sample statistic*, giving us a data frame containing 500 `sample means`. We will then see if these 500 means lie close to the `pop_mean`:

```{r}
#| layout: [[75], [25]]
#| warning: false

sample_50_500 <- do(500) * {
  sample(NHANES_adult, size = 50) %>%
    select(Height) %>% # drop sampling related column "orig.id"
    summarise(
      sample_mean = mean(Height),
      sample_sd = sd(Height),
      sample_min = min(Height),
      sample_max = max(Height))
}
sample_50_500
dim(sample_50_500)

```


```{r}
#| layout-ncol: 2

# Set graph theme
theme_set(new = theme_custom())
#

sample_50_500 %>%
  gf_point(.index ~ sample_mean, color = "purple",
           title = "Sample Means are close to the Population Mean",
           subtitle = "Sample Means are Random!",
           caption = "Grey lines represent our 500 samples") %>%
  
  gf_segment(
    .index + .index ~ sample_min + sample_max,
    color = "grey",
    linewidth = 0.3,
    alpha = 0.3,
    ylab = "Sample Index (1-500)",
    xlab = "Sample Means"
  ) %>%
  
  gf_vline(xintercept = ~ pop_mean, 
           color = "black") %>%
  
  gf_label(-25 ~ pop_mean, label = "Population Mean", 
           color = "black") 

##

sample_50_500 %>%
  gf_point(.index ~ sample_sd, color = "purple",
           title = "Sample SDs are close to the Population Sd",
           subtitle = "Sample SDs are Random!",) %>%
  
  gf_vline(xintercept = ~ pop_sd, 
           color = "black") %>%
  
  gf_label(-25 ~ pop_sd, label = "Population SD", 
           color = "black") %>% 
  gf_refine(lims(x = c(4,16)))

```

::: callout-note
### Sample Means and Sample SDs are a Random Variables!![^8]

The `sample_mean`s (purple dots in the two graphs), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the `pop_mean` (vertical black line). And hence they will have a `mean` and `sd` too `r emoji::emoji("scream")`. Do not get confused ;-D

And the `sample_sd`s are also random and [have their own distribution](https://www.researchgate.net/post/Standard_Deviation_Distribution/59032925ed99e1ba910f11ff/citation/download), around the `pop_sd`.[^5]
:::

[^8]: <https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/06%3A_Sampling_Distributions/6.01%3A_The_Mean_and_Standard_Deviation_of_the_Sample_Mean>

[^5]: <https://mathworld.wolfram.com/StandardDeviationDistribution.html>

### {{< iconify game-icons card-pickup >}} {{< iconify tabler chart-histogram >}} Distribution of Sample-Means

Since the **sample-means** are themselves random variables, let's plot
the **distribution** of these 500 sample-means themselves, called [**a distribution of sample-means**]{.bg-yellow .black}. We will also plot the position of the population mean `pop_mean` parameter, the mean of the `Height` variable.

```{r}
#| label: Sampling-Mean-Distribution
#| layout-ncol: 2
#| warning: false
#| fig-cap: Distributions
#| fig-subcap: 
#|  - Sample Means
#|  - Sample Means and Population

# Set graph theme
theme_set(new = theme_custom())
#

sample_50_500 %>% 
  gf_dhistogram(~ sample_mean, bins = 30, xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean, 
           color = "blue") %>% 
  
  gf_label(0.01 ~ pop_mean, 
            label = "Population Mean", 
            color = "blue") %>% 
gf_labs(title = "Sampling Mean Distribution",
        subtitle = "500 means")


# How does this **distribution of sample-means** compare with the
# overall distribution of the population?
# 
sample_50_500 %>% 
  gf_dhistogram(~ sample_mean, bins = 30,xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean, 
           color = "blue") %>% 
  
   gf_label(0.01 ~ pop_mean, 
            label = "Population Mean", 
            color = "blue") %>% 

  ## Add the population histogram
  gf_histogram(~ Height, data = NHANES_adult, 
               alpha = 0.2, fill = "blue", 
               bins = 30) %>% 
  
  gf_label(0.025 ~ (pop_mean + 20), 
           label = "Population Distribution", color = "blue") %>% 
gf_labs(title = "Sampling Mean Distribution", subtitle = "Original Population overlay")


```


### {{< iconify carbon concept >}} Deriving the Central Limit Theorem (CLT)

We see in the Figure above that

-   the *distribution of sample-means* is centered around the
    `pop_mean`. ( Mean of the sample means = `pop_mean`!! `r emoji::emoji("scream")`)
-   That the "spread" of the *distribution of sample means* is less than
    `pop_sd`. Curiouser and curiouser! But exactly how much is it?
-   And what is the kind of distribution?

One more experiment.

Now let's repeatedly sample `Height` and compute the sample-means, and
look at the resulting histograms. (We will deal with sample-standard-deviations shortly.) We will also use sample sizes of `c(8, 16, ,32, 64)` and generate 1000 samples each time, take the means and plot these 4 * 1000 means:

```{r}
#set.seed(12345)

samples_08_1000 <- do(1000) * mean(resample(NHANES_adult$Height, size = 08))

samples_16_1000 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))

samples_32_1000 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))

samples_64_1000 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))

# samples_128_1000 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))

# Quick Check
head(samples_08_1000)

```

```{r}
#| warning: false
#| eval: false
#| echo: false
# Now let's create separate Q-Q plots for the different sample sizes.

# Set graph theme
theme_set(new = theme_custom())
#

p1 <- gf_qq( ~ mean,data = samples_08_1000,
             title = "N = 8", 
             color = "dodgerblue") %>%
  gf_qqline()

p2 <- gf_qq( ~ mean,data = samples_16_1000,
            title = "N = 16", 
            color = "sienna") %>%
  gf_qqline()

p3 <- gf_qq( ~ mean,data = samples_32_1000,
            title = "N = 32", 
            color = "palegreen") %>%
  gf_qqline()

p4 <- gf_qq( ~ mean,data = samples_64_1000,
            title = "N = 64", 
            color = "violetred") %>%
  gf_qqline()

cowplot::plot_grid(p1, p2, p3, p4)

# The QQ plots show that the results become more normally distributed
# (i.e. following the straight line) as the samples get larger. 
```

Let us plot their individual histograms to compare them:

```{r}
#| warning: false
#| layout-ncol: 2
#| layout-nrow: 2

# Set graph theme
theme_set(new = theme_custom())
#
# Let us overlay their individual histograms to compare them:
p5 <- gf_dhistogram(~ mean,
              data = samples_08_1000,
              color = "grey",
              fill = "dodgerblue",title = "N = 8") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean, inherit = FALSE,
           color = "blue") %>%
  gf_label(-0.025 ~ pop_mean, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))
##
p6 <- gf_dhistogram(~ mean,
              data = samples_16_1000,
              color = "grey",
              fill = "sienna",title = "N = 16") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))
##
p7 <- gf_dhistogram(~ mean,
                    data = samples_32_1000 ,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "palegreen",title = "N = 32") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

p8 <- gf_dhistogram(~ mean, 
                    data = samples_64_1000,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "violetred",title = "N = 64") %>% 
  gf_fitdistr(linewidth = 1) %>% 
  gf_vline(xintercept = pop_mean,
         color = "blue") %>%
  gf_label(-.025 ~ pop_mean, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

#patchwork::wrap_plots(p5,p6,p7,p8)
p5
p6
p7
p8

```

And if we overlay these histograms on top of one another:

```{r, echo=FALSE}

# Set graph theme
theme_set(new = theme_custom())
#

gf_dhistogram(~ mean,
              data = samples_08_1000,
              color = "grey",
              fill = "dodgerblue",alpha = .5) %>% 
  # gf_label(0.15 ~ 172, inherit = FALSE,
  #          label = "N = 8", color = "dodgerblue") %>% 

gf_dhistogram(~ mean,
              data = samples_16_1000,
              color = "grey",
              fill = "sienna",alpha = 0.5) %>% 
    # gf_label(0.2 ~ 172, 
    #        label = "N = 16", color = "sienna") %>% 

gf_dhistogram(~ mean,
              data = samples_32_1000,
              color = "grey",
              fill = "palegreen",alpha = 0.5) %>% 
    # gf_label(0.3 ~ 172, 
    #        label = "N = 32", color = "palegreen") %>% 

gf_dhistogram(~ mean,
              data = samples_64_1000,
              color = "grey",
              fill = "violetred",alpha = 0.5) %>% 
    # gf_label(0.4 ~ 172, 
    #        label = "N = 64", color = "violetred") %>% 

gf_fitdistr(
  ~ mean,
  data = samples_08_1000,
  color = "dodgerblue",size = 2
) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_16_1000 ,
    color = "sienna",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_32_1000 ,
    na.rm = TRUE,
    color = "palegreen",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_64_1000 ,
    na.rm = TRUE,
    color = "violetred", size = 2
  ) %>%
  
  gf_vline(xintercept = pop_mean,
           color = "blue") %>%
  gf_label(-.02 ~ pop_mean, label = "Population Mean", 
           color = "blue") 

```

From the histograms we learn that the sample-means are **normally
distributed** around the *population mean*. [This feels intuitively right
because when we sample from the population, many values will be close to
the *population mean*, and values far away from the mean will be
increasingly scarce.]{style="background-color: yellow;"}

Let us calculate the means of the sample-distributions:

::::: columns
::: {.column width="60%"}
```{r}
#| label: sample-means
#| eval: false

mean(~ mean, data  = samples_08_1000) # Mean of means!!!;-0
mean(~ mean, data  = samples_16_1000)
mean(~ mean, data  = samples_32_1000)
mean(~ mean, data  = samples_64_1000)
pop_mean

```
:::

::: {.column width="40%"}
```{r}
#| ref.label: sample-means
#| echo: false
```
:::
:::::
All are pretty close to the population mean !!!


Consider the *standard deviations* of the sampling distributions:

$$
sd_i = \sqrt{\frac{\Sigma[{x_i - \bar{x_i}}]^2}{n_i}}
$$

::::: columns
::: column
We take the sum of all *squared [differences]{.bg-washed-red .black}* from the mean in a sample. If we divide this sum-of-squared-differences by the sample length $n$, we get a sort of *average squared difference*, or *per-capita* squared error from the mean, which is called the **variance**.

Taking the square root gives us an average error, which we would call, of course, the *standard deviation*.
:::

::: column
```{r}
#| label: showing-variance-calculation
#| echo: false

# Set graph theme
theme_set(new = theme_custom())
##
sample_50 %>% 
  rowid_to_column() %>% 
  gf_hline(yintercept = mean(~sample_50$Height)) %>% 
  gf_segment(mean(Height) + Height ~ rowid + rowid, colour = "red") %>% 
  gf_point(Height ~ rowid) %>% 
  gf_labs(x = "Observation Index", y = "Sampled Height",
  title = "Sum of Squares of Red lines/n = Variance",
subtitle = "Black Line = Mean")

```
:::
:::::


::::: columns
::: {.column width="60%"}
```{r}
#| label: sample-distribution-sds
#| eval: false

pop_sd
sd(~ mean, data  = samples_08_1000)
sd(~ mean, data  = samples_16_1000)
sd(~ mean, data  = samples_32_1000)
sd(~ mean, data  = samples_64_1000)

```
:::

::: {.column width="40%"}
```{r}
#| ref.label: sample-distribution-sds
#| echo: false
```
:::
:::::

These are also decreasing steadily with sample size. How are they related to the `pop_sd`?

::::: columns
::: {.column width="60%"}
```{r}
#| label: sample-dist-sds-popsd
#| eval: false

pop_sd
pop_sd /sqrt(8)
pop_sd / sqrt(16)
pop_sd / sqrt(32)
pop_sd / sqrt(64)

```
:::

::: {.column width="40%"}
```{r}
#| ref.label: sample-dist-sds-popsd
#| echo: false
```
:::
:::::

As we can see, the *standard deviations* of the sampling-mean-distributions is inversely proportional to the squre root of lengths of the sample.

::: callout-note
## Central Limit Theorem

Now we have enough to state the **Central Limit Theorem (CLT)**

-   the [sample-means are normally distributed around the *population
    mean*]{style="background-color: yellow;"}. So any mean of a **single sample** is a good, **unbiased estimate** for the `pop_mean`
-   the **sample-mean distributions** narrow with sample length, i.e [their `sd` decreases with increasing sample size.]{style="background-color: yellow;"}
-   $sd \sim \frac{1}{sqrt(n)}$
-   This is [regardless of the distribution of the *population*]{style="background-color: yellow;"} itself.[^1]
:::

[^1]: The \`Height\` variable seems to be normally distributed at
    population level. We will try other non-normal population variables
    as an exercise in the tutorials.

This theorem underlies all our procedures and techniques for statistical inference, as we shall see.

{{< video https://youtu.be/_YOr_yYPytM >}}


## {{< iconify dashicons code-standards >}} The Standard Error

Consider once again the *standard deviations* in each of the sample-distributions that we have generated. As we saw these decrease with sample size.

`sd = pop_sd/sqrt(sample_size)`where sample-size[^2]  here is
one of `c(8, 16, 32, 64)`.

[^2]: Once `sample size = population`, we have complete access to the
    population and there is no question of estimation error! So
    `sample_sd = pop_sd`!

We reserve the term *Standard Deviation* for the population, and name
this computed standard deviation of the sample-mean-distributions as the
**Standard Error**. This statistic derived from the sample-mean-distribution will help us infer our population parameters with a precise estimate of the *uncertainty* involved.

$$
Standard\ Error\ \pmb {se} = \frac{pop.sd}{\sqrt[]{n}}\\
$$
[However, we don't normally know the `pop_sd`!! So now what?? So is this chicken and egg??]{style="background-color: yellow;"}

Let us take **SINGLE SAMPLES** of each size, as we normally would:

```{r}
#| label: single-samples
sample_08 <- mosaic::sample(NHANES_adult, size = 8) %>% 
  select(Height)
sample_16 <- mosaic::sample(NHANES_adult, size = 16) %>% 
  select(Height)
sample_32 <- mosaic::sample(NHANES_adult, size = 32) %>% 
  select(Height)
sample_64 <- mosaic::sample(NHANES_adult, size = 64) %>% 
  select(Height)
##
sd(~ Height, data = sample_08)
sd(~ Height, data = sample_16)
sd(~ Height, data = sample_32)
sd(~ Height, data = sample_64)
  
```

::: callout-warning
As we saw at the start of this module, the sample-means are distributed around the `pop_mean`, AND it appears that the sample-sds are also distributed around the `pop_sd`! 

However, the sample-sd distribution is **not Gaussian**, and the sample_sd is also **not an unbiased estimate** for the `pop_sd`. However, when the sample size $n$ is large ($n >= 30$), the sample-sd distribution approximates the Gaussian, and the bias is small. 
:::

And so, in the same way, [we approximate the `pop_sd` with the sd of a **single sample** of the same length]{.bg-washed-red .black}. Hence the *Standard Error* can be computed from **a single sample** as:

$$
Standard\ Error\ \pmb {se} = \frac{sample.sd}{\sqrt[]{n}}
$$

With these, the Standard Errors(SE) evaluate to:

::::: columns
::: {.column width="60%"}

```{r}
#| label: standard-error-computation
#| eval: false
pop_sd <- sd(~ Height, data = NHANES_adult)
pop_sd
sd(~ Height, data = sample_08) / sqrt(8)
sd(~ Height, data = sample_16) / sqrt(16)
sd(~ Height, data = sample_32) / sqrt(32)
sd(~ Height, data = sample_64) / sqrt(64)

```
:::

::: {.column width="40%"}
```{r}
#| ref.label: standard-error-computation
#| echo: false
```
:::
:::::


## {{< iconify fluent auto-fit-width-24-filled >}} Confidence intervals

When we work with samples, we want to be able to speak with a certain
degree of confidence about the **population mean**, based on the
evaluation of **one** sample mean, not a large number of them. Given
that sample-means are *normally distributed* around the `pop_mean`, we can say that $68\%$ of *all possible sample-mean* lie within
$\pm SE$ of the *population mean*; and further that $95 \%$ of *all
possible sample-mean* lie within $\pm 2*SE$ of the *population mean*. 

These two intervals $sample.mean \pm SE$ and $sample.mean \pm 2*SE$ are
called the **confidence intervals** for the population mean, at levels
$68\%$ and $95 \%$ probability respectively.

```{r}
# Set graph theme
theme_set(new = theme_custom())
#
tbl_1 <- get_ci(samples_08_1000, level = 0.95)
tbl_2 <- get_ci(samples_16_1000, level = 0.95)
tbl_3 <- get_ci(samples_32_1000, level = 0.95)
tbl_4 <- get_ci(samples_64_1000, level = 0.95)
rbind(tbl_1, tbl_2, tbl_3, tbl_4) %>% 
  rownames_to_column("index") %>% 
cbind("sample_size" = c(8,16,32,64)) %>% 
gf_segment(index + index ~ lower_ci + upper_ci) %>% 
gf_vline(xintercept = pop_mean) %>% 
gf_labs(title = "95% Confidence Intervals for the Mean",
        subtitle = "Varying samples sizes 8-16-32-64",
        y = "Sample Size", x = "Mean Ranges") %>% 
gf_refine(scale_y_discrete(labels = c(8,16,32,64))) %>% 
gf_refine(annotate(geom = "label", x = pop_mean + 1.75, y = 1.5, label = "Population Mean"))

```

::: callout-important
## Confidence Intervals Shrink!

Yes, with sample size! Why is that a good thing?
:::

So finally, [we can *pretend* that our sample is also distributed normally ( i.e Gaussian)]{style="background-color: yellow;"} and use its `sample_sd` as a substitute for `pop_sd`, plot the `standard errors` **on the sample histogram**, and see where our *believed mean* is. 


```{r}
#| label: Final-approximation-pop-sample
#| message: true
#| results: hold


# Set graph theme
theme_set(new = theme_custom())
#
sample_mean = mean(~ Height, data = sample_16)
se = sd(~ Height, data = sample_16)/sqrt(16)
#
xqnorm(p = c(0.025, 0.975), 
        mean = sample_mean,
        sd = sd(~ Height, data = sample_16),
        return = c("plot"), verbose = F) %>% 
gf_vline(xintercept = ~ pop_mean, colour = "black") %>% 
gf_vline(xintercept = mean(~ Height, data = sample_16), colour = "purple") %>% 
gf_labs(title = "Confidence Intervals and the Bell Curve. N=16",
        subtitle = "Sample is plotted as theoretical Gaussian Bell Curve") %>% 
gf_refine(annotate(geom = "label", x = pop_mean + 15, y = 0.05, label = "Population Mean"),
annotate(geom = "label", x = sample_mean - 15, y = 0.05, label = "Sample Mean", colour = "purple")
)

```


```{r}
#| label: confidence-interval-computation-1
#| results: hold
pop_mean
se <- sd(~ Height, data = sample_16) / sqrt(16)
mean(~ Height, data = sample_16) - 2.0 * se
mean(~ Height, data = sample_16) + 2.0 * se

```

So if our *believed mean* is within the Confidence Intervals, then OK, we can say our belief may be true. If it is way outside the confidence intervals, we have to think that our belief may be flawed and accept and alternative interpretation. 

## {{< iconify flat-color-icons workflow >}} CLT Workflow

Thus if we want to estimate a *population mean*:

-   we take one **random** sample from the population of length $n$
-   we calculate the mean from the sample `sample-mean`
-   we calculate the `sample-sd`
-   we calculate the *Standard Error* as $\frac{sample-sd}{\sqrt[]{n}}$
-   we calculate 95% confidence intervals for the *population parameter* based on the formula
    $CI_{95\%}= sample.mean \pm 2*SE$.
-   Since Standard Error decreases with sample size, we need to make our sample of adequate size.( $n=30$ seems appropriate in most cases. Why?)
-   And we do not have to worry about the distribution of the
    population. It need not be normal/Gaussian/Bell-shaped !!
    
## {{< iconify flat-color-icons workflow >}} CLT Assumptions

- Sample is of "decent length" $n >= 30$;
- Therefore sample histogram is Gaussian

## {{< iconify material-symbols interactive-space-outline >}} An interactive Sampling app

Here below is an interactive sampling app. Play with the different
settings, especially the *distribution in the population* to get a firm
grasp on Sampling and the CLT.

<https://gallery.shinyapps.io/CLT_mean/>

## {{< iconify ooui references-rtl >}} References

1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,
    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>
2.  Stats Test Wizard.
    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>
3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:
    *OpenIntro Statistics*. Available online
    <https://www.openintro.org/book/os/>

4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring
    data to inference and predictive modelling*
    <http://www.modernstatisticswithr.com/>

5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear
    models (or: how to teach stats)
    <https://lindeloev.github.io/tests-as-linear/>

6.  CheatSheet
    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>

7.  Common statistical tests are linear models: a work through by Steve
    Doogue <https://steverxd.github.io/Stat_tests/>

8.  Jeffrey Walker "Elements of Statistical Modeling for Experimental
    Biology".
    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>

9.  Adam Loy, Lendie Follett & Heike Hofmann (2016) Variations of
    *Q*--*Q* Plots: The Power of Our Eyes!, The American Statistician,
    70:2, 202-214, DOI:
    [10.1080/00031305.2015.1077728](https://doi.org/10.1080/00031305.2015.1077728)

::: {#refs style="font-size: 60%;"}

###### {{< iconify lucide package-check >}} R Package Citations

```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("NHANES", "regressinator", "smovie", "TeachHist",
           "TeachingDemos", "visualize")
) %>%
  knitr::kable(format = "simple")

```
:::


