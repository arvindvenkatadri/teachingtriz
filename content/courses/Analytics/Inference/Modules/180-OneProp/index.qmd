---
title: "\U0001F0CF Testing a Single Proportion"
date: 10/Nov/2022
date-modified: "`r Sys.Date()`"
abstract: "Inference Tests for the significance of a Proportion"
order: 180
image: preview.jpg
image-alt: From The Internet Archive
categories:
- Permutation
- Monte Carlo Simulation
- Random Number Generation
- Distributions
- Generating Parallel Worlds
bibliography: 
  - grateful-refs.bib
citation: true
---

## {{< iconify noto-v1 package >}} Setting up R packages

```{r}
#| label: setup
#| include: true
#| message: false
#| warning: false
library(tidyverse)
library(mosaic)
library(ggformula)
library(infer)

## Datasets from Chihara and Hesterberg's book (Second Edition)
library(resampledata)

## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)
library(openintro)

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
#devtools::install_github("nicolash2/ggbrace")
library(ggbrace)
library(TeachHist)
library(TeachingDemos)
library(grateful)

```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

Often we hear reports that a certain percentage of people support a certain political party, or that a certain proportion of people are in favour of a certain policy. Such statements are the result of a desire to infer a *proportion in the population*, which is what we will investigate here. 


## {{< iconify flat-color-icons workflow >}} Workflow: Sampling Theory for Proportions

We have seen how sampling from a population works when we wish to estimate **means**:

  - The sample means $\bar{x}$ are centred around the population mean $\mu$;
  - The samples means are *normally distributed*
  - The uncertainty in using $\bar{x}$ as an estimate for $\mu$ is given by a Confidence interval defined by some constant times the Standard Error of the sample $\frac{s}{\sqrt(n)}$;
  - The larger the size of the sample, the tighter the Confidence Interval.
  
Now then: does a similar logic work for `proportions` too, as for `means`?

### {{< iconify flat-color-icons workflow >}} The CLT for Proportions
 - Sample proportions are also centred around population proportions
 - **Success-failure condition**: If $\hat{p} *n >= 10$ and $(1-\hat{p})*n >= 10$ are both satisfied, then the we can assume that the *sampling distribution* of the proportion is normal. And so:
 - The Standard Error for a sample proportion is given by $SE = \sqrt\frac{\hat{p}(1-\hat{p})}{n}$, where $\hat{p}$ is the sample proportion
 - We would calculate the Confidence Intervals in a similar fashion, based on the desired probability of error, as:
 
$$
p = \hat{p} \pm 1.96*{SE}
$$


## {{< iconify pajamas issue-type-test-case >}} Case Study #1: YRBSS Survey

We will be analyzing the same dataset called the *Youth Risk Behavior Surveillance System (YRBSS) survey* from the `openintro` package, which uses data from high schoolers to help discover health patterns. The dataset is called `yrbss`.

### {{< iconify flat-color-icons workflow >}} Workflow: Read the Data

```{r}
#| label: read_yrbss
data(yrbss, package = "openintro")
yrbss
```

When summarizing the YRBSS data, the Centers for Disease Control and Prevention seeks insight into the population *parameters*. Accordingly, in this tutorial, our research questions are:

::: callout-note
## Research Questions
1.  What are the counts within each category for the amount of days these students have texted while driving within the past 30 days?

2. What proportion of people on earth have texted while driving each day for the past 30 days without wearing helmets?
:::

Question 1 pertains to the data set `yrbss`, our "sample". To answer this, you can answer the question, "What proportion of people in your sample reported that they have texted while driving each day for the past 30 days?" with a statistic. Question 2 is an inference we need to make about the *population of highschoolers*.  While the question "What proportion of people on earth have texted while driving each day for the past 30 days?" is answered with an estimate of the parameter.

For our first Research Question, we will choose the column `helmet_12m`: Remember that you can use `filter` to limit the dataset to just non-helmet wearers. Here, we will name the (filtered ) dataset `no_helmet`.

```{r}
#| label: no-helmet-text
yrbss %>% 
  group_by(helmet_12m) %>% count()
##
yrbss %>% 
  group_by(text_while_driving_30d) %>% count()
```

Also, it may be easier to calculate the proportion if we create a new variable that specifies whether the individual has *texted every day* while driving over the past 30 days or not. We will call this variable `text_ind`.

```{r}
no_helmet_text <- yrbss %>%
  filter(helmet_12m == "never") %>% 
  mutate(text_ind = ifelse(text_while_driving_30d == "30", "yes", "no")) %>% 
  # removing most of the other variables
  select(age, gender, text_ind)
no_helmet_text
##
no_helmet_text %>% drop_na() %>% count(text_ind)
##
no_helmet_text %>% drop_na() %>% 
  summarize(prop = prop(text_ind, success = "yes"), n = n())

```
This is the `observed_statistic`: the proportion of people  **in this sample** who do text when they drive without a helmet.

### Visualizing a Single Proportion

We can quickly plot this, just for the sake of visual understanding of the proportions:

```{r}
# Set graph theme
theme_set(new = theme_custom())
#
no_helmet_text %>% drop_na() %>% 
  gf_bar(~ text_ind) %>% 
  gf_labs(x = "texted?",
          title = "High-Schoolers who texted every day",
          subtitle = "While driving with no helmet on!!")
```

## Inference for a Single Proportion

Based on this **sample** in the `yrbss` data, we wish to infer proportions for the **population** of high-schoolers. 


### Hypothesis Testing for a Single Proportion

Consider the inference we did for a single mean. What was our NULL Hypothesis? That the population mean $\mu = 0$. for two means? That they might be equal. What might a suitable NULL Hypothesis be for a *single proportion*? What *attitude* of [ain't nothing happenin']{.bg-yellow .black} might we adopt?

::: callout-important
With proportions, we usually look for a "no difference" situation, i.e. a ratio of unity!! So our NULL hypothesis would be a proportion of 1:1 for texters and no-texters, so a proportion of $0.5$!!
:::

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}

### Classical Test

The simplest test in R for a single proportion is the `binom.test`:

```{r}
mosaic::binom.test(~ text_ind, data = no_helmet_text, success = "yes") 
mosaic::binom.test(~ text_ind, data = no_helmet_text, success = "yes") %>% 
  broom::tidy()
```

How do we understand this result? That the sample tells us the $\hat{p} = 0.07119$ and that based on this the population proportion of those who text while driving without a helmet is also **not 0.5**, since the `p-value` is $2.2e-16$. So we reject the NULL hypothesis and accept the alternative hypothesis. 

### Uncertainty in Estimation

The Confidence Intervals from the `binom.test` inform us about our population proportion estimate: It lies within the interval [0.06506429, 0.07771932]. We know that this is also given by:


$$
\begin{eqnarray}

CI &=& \hat{p} ~ \pm 1.96*SE\\
&=& \hat{p} ~ \pm 1.96*\sqrt{\hat{p}* (1-\hat{p})/n}\\
&=& 0.0711 \pm 1.96*\sqrt{0.0711 * (1- 0.0711)/6847}\\
&=& 0.0711 \pm 0.006\\
&=& [0.065, 0.771]
\end{eqnarray}
$$

::: {.hidden}
### {{< iconify svg-spinners blocks-shuffle-3 >}} Permutation Visually Demonstrated 

We saw from the diagram created by Allen Downey that [there is only one test](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html)! We will now use this philosophy to develop a technique that allows us to
mechanize several *Statistical Models* in that way, with nearly identical code.
We will first look visually at a permutation exercise. We will create dummy data that contains the following case study:

> A set of identical resumes was sent to male and female evaluators. The
> candidates in the resumes were of both genders. We wish to see if
> there was difference in the way resumes were evaluated, by male and
> female evaluators. (We use just *one* male and *one* female evaluator
> here, to keep things simple!)


```{r}
#| label: Artificial Data
#| layout-ncol: 2
#| echo: false
#| column: body-outset-right

set.seed(123456)
data1 <- tibble(evaluator = rep(x = "F", times = 24),
               candidate_selected = sample(c(0,1), 
                               size = 24, 
                               replace = T, 
                                prob = c(0.1, 0.9)))

data2 <- tibble(evaluator = rep(x = "M", times = 24),
               candidate_selected = sample(c(0,1), size = 24, 
                                replace = T, 
                                prob = c(0.6, 0.4)))

#data1
#data2
               
data <-  rbind(data1, data2) %>%
  as_tibble() %>%

  # Create a 4*12 matrix of integer coordinates
  cbind(expand.grid(x = 1:4, y = seq(4, 48,4))) %>%
  mutate(evaluator = as_factor(evaluator))

data %>% 
  select(evaluator, candidate_selected) %>% 
  as_tibble()

summary <- data %>%
  group_by(evaluator) %>%
  summarise(selection_ratio = mean(candidate_selected == "0"),
            count = count(candidate_selected == "0"),
            n = n())
summary

```





```{r}
#| label: Difference in Proportion
#| layout: [[20],[80]]
#| echo: false
#| warning: false
# Set graph theme
theme_set(new = theme_custom())
#
obs_difference <-
  diff(mean(candidate_selected ~ evaluator, data = data))
obs_difference

###
p0 <- data %>%
  
  # knock off the coordinates prior to shuffle
  select(evaluator, candidate_selected) %>%
  #mutate(candidate_selected = shuffle(candidate_selected, replace = FALSE)) %>%
  arrange(evaluator, candidate_selected) %>%
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24, 4))) %>% # Not 48!!
  
  ggplot(data = ., aes(
    x = x,
    y = y,
    group = evaluator,
    fill = as_factor(candidate_selected)
  )) +
  geom_point(shape = 21, size = 8) +
  scale_fill_manual(
    name = NULL,
    values = c("red", "blue"),
    labels = c("Rejected", "Selected")
  ) +
  
  # Very important to have scales = "free" !!
  facet_wrap( ~ evaluator, ncol = 1, scales = "free") +
  
  theme_void() +
  theme(
    legend.position = "top",
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    plot.title = element_text(hjust = 0.5)
  ) +
  expand_limits(y = c(0, 28)) +
  
  # Need to give stat_brace two pairs x values and y values
  # as there are two facets
  ggbrace::stat_brace(
    aes(#x = c(4.5, 5.5, 4.5, 5.5),
      # What a hack this is!!
      #y = c(4, 24, 5, 24),
      label = evaluator),
    rotate = 90,
    labelrotate = 0,
    labelsize = 4,
    inherit.aes = TRUE
  )
p0

```

So, we have a solid disparity in percentage of selection between the two evaluators! Now we pretend that *there is no difference between the selections made by either set of evaluators*. So we can just:

-   Pool up all the evaluations\
-   Arbitrarily re-assign a given candidate(selected or rejected) to
    either of the two sets of evaluators, by permutation.\

How would that pooled shuffled set of evaluations look like?

```{r}
#| label: Shuffle the data once
#| echo: false

data_shuffled <- data %>%
  mutate(evaluator = shuffle(evaluator))
#data_shuffled %>% select(evaluator, candidate)
data_shuffled %>%
  group_by(evaluator) %>%
  summarise(
    selection_ratio = mean(candidate_selected == "0"),
    count = count(candidate_selected == "0"),
    n = n()
  )
```

```{r}
#| echo: false
#| layout: [[45,-10, 45]]
#| warning: false
data_shuffled %>% 
      group_by(evaluator, candidate_selected) %>% 
      ggplot(aes(x = x, y = y, 
                 group = evaluator, 
                 fill = as_factor(candidate_selected))) + 
      geom_point(shape = 21,size = 8) + 
      scale_fill_manual(name = NULL,
                        values = c("red", "blue"),
                        labels = c("Rejected","Selected")) + 
      theme_void() + theme(legend.position = "top") +
      ggtitle(label = "Pooled Evaluations, Permuted")


data_shuffled %>% 
  
  # knock off the coordinates
  # Do not use "group_by"!! Why not?
  select(evaluator, candidate_selected) %>% 
  arrange(evaluator,candidate_selected) %>% 
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24,4))) %>% # Not 48!!
  
  ggplot(data = ., aes(x = x, y = y, 
             group = evaluator, 
             fill = as_factor(candidate_selected))) + 
  geom_point(shape = 21, size = 8) + 
  scale_fill_manual(name = NULL,
                    values = c("red", "blue"),
                    labels = c("Rejected","Selected")) + 
  
  # Very important to have scales = "free" !!
  facet_wrap(~ evaluator, ncol = 1, scales = "free") +
   expand_limits(y = c(0,28)) +

  ggbrace::stat_brace(aes(label = evaluator),
                      rotate = 90,
                      labelrotate = 0,
                      labelsize = 4,
                      inherit.aes = TRUE) +
  theme_void() + 
  theme(legend.position = "top", 
                       strip.background = element_blank(),
                       strip.text.x = element_blank(),
                       plot.title = element_text(hjust = 0.5)) +
  ggtitle(label = "Permuted Evaluations, Grouped")

```

```{r}
#| label: Old code retained
#| echo: false
#| message: false
#| warning: false
#| eval: false

library(patchwork)
p1 + p2 + plot_annotation(tag_levels = "A") + plot_layout(widths = unit(c(7, 10), c('cm', 'cm')))

```

As can be seen, the ratio is different! 

We can now check out our Hypothesis that there is *no* bias. We can shuffle the data many many times, calculating the ratio each time, and plot the *distribution of the differences in selection ratio* and see how that artificially created distribution compares with the originally observed figure from Mother Nature.

```{r}
#| layout: [[60, -10, 30]]
# Set graph theme
theme_set(new = theme_custom())
#
null_dist <- do(4999) * diff(mean(
  candidate_selected ~ shuffle(evaluator), 
  data = data))
# null_dist %>% names()
null_dist %>% gf_histogram( ~ M, 
                  fill = ~ (M <= obs_difference), 
                  bins = 25,show.legend = FALSE,
                  xlab = "Bias Proportion", 
                  ylab = "How Often?",
                  title = "Permutation Test on Difference between Groups",
                  subtitle = "") %>% 
  gf_vline(xintercept = ~ obs_difference, color = "red" ) %>% 
  gf_label(500 ~ obs_difference, label = "Observed\n Bias", 
           show.legend = FALSE) 

mean(~ M<= obs_difference, data = null_dist)

```

We see that the artificial data can hardly ever ($p = 0.0022$) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.

:::

### {{< iconify mdi cards-playing-outline >}} Bootstrap test

The inferential tools for estimating a single population proportion are analogous to those used for estimating single population means: the bootstrap confidence interval and the hypothesis test.

```{r}
#| label: nohelmet-text-ci
no_helmet_text %>%
  drop_na() %>% 
  specify(response = text_ind, success = "yes") %>%
  generate(reps = 999, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)
```

Note that since the goal is to construct an interval estimate for a proportion, it's necessary to both include the `success` argument within `specify`, which accounts for the proportion of non-helmet wearers than have consistently texted while driving the past 30 days, in this example, and that `stat` within `calculate` is here "prop", signaling that we are trying to do some sort of inference on a proportion.


:::

## {{< iconify pajamas issue-type-test-case >}} Case Study #2: TBD

To be Written up in the foreseeable future. 

## {{< iconify material-symbols interactive-space-outline >}} An interactive app

<https://openintro.shinyapps.io/CLT_prop/>

## {{< iconify mingcute thought-line >}} Wait, But Why?
- In business, or "design research", one encounters things that are proportions in a target population:
  - Adoption of a service or an app
  - People preferring a particular product
  - Beliefs which are of Yes/No type: Is this Govt. doing the right thing with respect to taxes?
  - Knowing what this population proportion is a necessary step to take a decision about what you will do about it. 
  - (Other than plot a *&^%#$$%^^& pie chart)

## {{< iconify fluent-mdl2 decision-solid >}} Conclusion

- We have seen how the CLT works with proportions, in a manner similar to that with means
- The Standard Error (and therefore the CI) for the inference of a *proportion* is **related to the actual population proportion**, which is very different behaviour from that with means, where SE was just a number that depended on the sample size
- Bootstrap procedures work with inference for a single proportion. (Permutation when there are two)

## {{< iconify bi person-up >}} Your Turn

1. Type `data(package = "resampledata")` and `data(package = "resampledata3")` in your RStudio console. This will list the datasets in both these package. Try loading a few of these and infering for single proportions. 

2. National Health and Nutrition Examination Survey (NHANES) dataset. Install the package `NHANES` and explore the dataset for proportions that might be interesting. 


## {{< iconify ooui references-rtl >}} References

1. StackExchange. *`prop.test` vs `binom.test` in R*. <https://stats.stackexchange.com/q/551329>

1. Mine Çetinkaya-Rundel and Johanna Hardin, [OpenIntro Modern Statistics: Chapter 17](https://openintro-ims.netlify.app/inference-one-prop.html)\
2. Laura M. Chihara, Tim C. Hesterberg, [*Mathematical Statistics with Resampling and R*. 3 August 2018.© 2019 John Wiley & Sons, Inc.](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119505969)\

1. OpenIntro Statistics Github Repo: <https://github.com/OpenIntroStat/openintro-statistics> 


::: {#refs style="font-size: 60%;"}
###### {{< iconify lucide package-check >}} R Package Citations
```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("ggbrace", "openintro", "resampledata")
) %>%
  knitr::kable(format = "simple")

```

:::

