---
title: "Inference for Correlation"
author: "Arvind V."
date: 25/Nov/2022
date-modified: "`r Sys.Date()`"
order: 140
keywords: Statistics ; Tests; p-value
abstract: "Statistical Significance Tests for Correlations between two Variables"
bibliography: 
  - grateful-refs.bib
citation: true
#suppress-bibliography: true
editor: 
  markdown: 
    wrap: 72
---

## {{< iconify noto-v1 package >}} Setting up R packages

```{r}
#| label: setup
#| warning: false
#| message: false
#| include: true

# CRAN Packages
library(tidyverse)
library(mosaic)
library(ggformula)
library(broom)
library(mosaicCore)
library(mosaicData)
library(crosstable) # tabulated summary stats

library(openintro) # datasets and methods
library(resampledata3) # datasets
library(statsExpressions) # datasets and methods
library(ggstatsplot) # special stats plots
library(ggExtra)

# Non-CRAN Packages
# remotes::install_github("easystats/easystats")
library(easystats)

```

```{r}
#| label: Extra-Pedagogical-Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(grateful)

```


#### Plot Fonts and Theme

```{r}
#| label: plot-theme
#| echo: true
#| code-fold: true
#| messages: false
#| warning: false

library(systemfonts)
library(showtext)
## Clean the slate
systemfonts::clear_local_fonts()
systemfonts::clear_registry()
##
showtext_opts(dpi = 96) #set DPI for showtext
sysfonts::font_add(family = "Alegreya",
  regular = "../../../../../../fonts/Alegreya-Regular.ttf",
  bold = "../../../../../../fonts/Alegreya-Bold.ttf",
  italic = "../../../../../../fonts/Alegreya-Italic.ttf",
  bolditalic = "../../../../../../fonts/Alegreya-BoldItalic.ttf")

sysfonts::font_add(family = "Roboto Condensed", 
  regular = "../../../../../../fonts/RobotoCondensed-Regular.ttf",
  bold = "../../../../../../fonts/RobotoCondensed-Bold.ttf",
  italic = "../../../../../../fonts/RobotoCondensed-Italic.ttf",
  bolditalic = "../../../../../../fonts/RobotoCondensed-BoldItalic.ttf")
showtext_auto(enable = TRUE) #enable showtext
##
theme_custom <- function(){ 
    font <- "Alegreya"   #assign font family up front
    
    theme_classic(base_size = 14, base_family = font) %+replace%    #replace elements we want to change
    
    theme(
      text = element_text(family = font),  #set base font family
      
      #text elements
      plot.title = element_text(                 #title
                   family = font,          #set font family
                   size = 24,                    #set font size
                   face = 'bold',                #bold typeface
                   hjust = 0,                    #left align
                   margin = margin(t = 5, r = 0, b = 5, l = 0)), #margin
      plot.title.position = "plot", 
      
      plot.subtitle = element_text(              #subtitle
                   family = font,          #font family
                   size = 14,                   #font size
                   hjust = 0,                   #left align
                   margin = margin(t = 5, r = 0, b = 10, l = 0)), #margin
      
      plot.caption = element_text(               #caption
                   family = font,          #font family
                   size = 9,                     #font size
                   hjust = 1),                   #right align
      
      plot.caption.position = "plot",            #right align
      
      axis.title = element_text(                 #axis titles
                   family = "Roboto Condensed",  #font family
                   size = 12),                   #font size
      
      axis.text = element_text(                  #axis text
                   family = "Roboto Condensed",  #font family
                   size = 9),                    #font size
      
      axis.text.x = element_text(                #margin for axis text
                    margin = margin(5, b = 10))
      
      #since the legend often requires manual tweaking 
      #based on plot content, don't define it here
    )
}

## Use available fonts in ggplot text geoms too!
update_geom_defaults(geom = "text",new = list(
  family = "Roboto Condensed",
  face = "plain",
  size = 3.5,
  color = "#2b2b2b"
)
)

## Set the theme
theme_set(new = theme_custom())

```


## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

Correlations define how one variables varies with another. One of the basic Questions we would have of our data is: Does some variable have a significant correlation score with another in some way? Does $y$ vary with $x$? A **Correlation Test** is designed to answer exactly this question.  The block diagram depicts the statistical procedures available to test for the significance of correlation scores between two variables. 

Before we begin, let us recap a few basic definitions:

We have already encountered the `variance` of a variable:

$$
\begin{align*}
var_x &= \frac{\sum_{i=1}^{n}(x_i - \mu_x)^2}{(n-1)}\\
where ~ \mu_x &= mean(x)\\
n &= sample\ size
\end{align*}
$$
The *standard deviation* is:

$$
\sigma_x = \sqrt{var_x}\\
$$
The **covariance** of two variables is defined as:

$$
\begin{align}
cov(x,y) &= \frac{\sum_{i = 1}^{n}(x_i - \mu_x)*(y_i - \mu_y)}{n-1}\\
&= \frac{\sum{x_i *y_i}}{n-1} - \frac{\sum{x_i *\mu_y}}{n-1} - \frac{\sum{y_i *\mu_x}}{n-1} + \frac{\sum{\mu_x *\mu_y}}{n-1}\\
&= \frac{\sum{x_i *y_i}}{n-1} - \frac{\sum{\mu_x *\mu_y}}{n-1}\\
\end{align}
$$

Hence covariance is *the expectation of the product minus the product of the expectations* of the two variables. 

::: callout-tip
### Covariance uses [z-scores](../../../Descriptive/Modules/22-Histograms/index.qmd)!
Note that in both cases we are dealing with **z-scores**: variable minus its mean, $x_i - \mu_x$, which we have seen when dealing with the CLT and the Gaussian Distribution. 
:::

So, finally, the coefficient of **correlation** between two variables is defined as:

$$
\begin{align}
correlation ~ r &= \frac{cov(x,y)}{\sigma_x * \sigma_y}
\\
&= \frac{cov(x,y)}{\sqrt{var_x} * \sqrt{var_y}}
\end{align}
$${#eq-corr}

Thus correlation coefficient is the *covariance scaled by the geometric mean of the variances*. 

```{mermaid}
%%| echo: false
flowchart TD
    A[Inference for Correlation] -->|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\n Variances: Fisher F-test var.test]
    B --> C{OK?}
    C -->|Yes, both\n Parametric| D[t.test]
    D <-->F[Linear Model\n Method] 
    C -->|Yes, but not variance\n Parametric| W[t.test with\n Welch Correction]
    W<-->F
    C -->|No\n Non-Parametric| E[wilcox.test]
    E <--> G[Linear Model\n with\n Ranked Data]
    C -->|No\n Non-Parametric| P[Bootstrap\n or\n Permutation]
    P <--> Q[Linear Model\n with Ranked Data\n and Permutation]
```
 
## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Galton's famous dataset

How can we start, except by using the famous `Galton` dataset, now part of the `mosaicData` package?

### {{< iconify flat-color-icons workflow >}} Workflow: Read and Inspect the Data

```{r}
#| label: read-galton
data("Galton", package = "mosaicData")
Galton
```
The variables in this dataset are:

::: callout-note
### Qualitative Variables

- `sex`(char): sex of the child
- `family`(int): an ID for each family
:::

::: callout-note
### Quantitative Variables

- `father`(dbl): father's height in inches
- `mother`(dbl): mother's height in inches
- `height`(dbl): Child's height in inches
- `nkids`(int): Number of children in each family
:::

```{r}
inspect(Galton)
```

So there are several correlations we can explore here: Children's `height` vs that of `father` or `mother`, based on `sex`. In essence we are replicating Francis Galton's famous study.


### {{< iconify flat-color-icons workflow >}} Workflow: Research Questions

::: callout-note
## Question 1
1.  Based on this sample, what can we say about the correlation between a son's height and a father's height in the population?
:::

::: callout-note
## Question 2
1.  Based on this sample, what can we say about the correlation between a daughter's height and a father's height in the population?
:::
Of course we can formulate more questions, but these are good for now! And since we are going to infer correlations by `sex`, let us split the dataset into two parts, one for the sons and one for the daughters, and quickly summarise them too:

```{r}
Galton_sons <- Galton %>% 
  dplyr::filter(sex == "M") %>% 
  rename("son" = height)
Galton_daughters <- Galton %>% 
  dplyr::filter(sex == "F") %>% 
  rename("daughter" = height)
dim(Galton_sons)
dim(Galton_daughters)
```


```{r}
Galton_sons %>% 
  summarize(across(.cols = c(son, father), 
                   .fns = list(mean = mean, sd = sd)))

Galton_daughters %>% 
  summarize(across(.cols = c(daughter, father), 
                   .fns = list(mean = mean, sd = sd)))
```

### {{< iconify flat-color-icons workflow >}} Workflow: Visualization

Let us first quickly plot a graph that is relevant to each of the two research questions.

```{r}
#| layout-ncol: 2
# Set graph theme
theme_set(new = theme_custom())
#
Galton_sons %>% 
  gf_point(son ~ father) %>% 
  gf_lm() %>% 
  gf_labs(title = "Height of Sons vs fathers",
          subtitle = "Galton dataset")
##
Galton_daughters %>% 
  gf_point(daughter ~ father) %>% 
  gf_lm() %>% 
  gf_labs(title = "Height of Daughters vs Fathers",
          subtitle = "Galton dataset")
```

We might even plot the **overall** heights together and colour by `sex` of the child:

```{r}
# Set graph theme
theme_set(new = theme_custom())
#
Galton %>% 
  gf_point(height ~ father, 
           group = ~ sex, colour = ~ sex) %>% 
  gf_lm() %>% 
  gf_refine(scale_color_brewer(palette = "Set1")) %>%
  gf_labs(title = "Height of sons vs fathers",
          subtitle = "Galton dataset")

```

So daughters are shorter than sons, generally speaking, and both heights seem related to that of the father. 

::: callout-note
### What did filtering do?
When we filtered the dataset into two, the filtering by `sex` of the child also effectively filtered the heights of the `father` (and `mother`). This is proper and desired; but think!

:::

### {{< iconify flat-color-icons workflow >}} Workflow: Assumptions

For the *classical* correlation tests, we need that the variables are normally distributed. As before we check this with the `shapiro.test`:

```{r}
#| layout-ncol: 2
shapiro.test(Galton_sons$father)
shapiro.test(Galton_sons$son)
##
shapiro.test(Galton_daughters$father)
shapiro.test(Galton_daughters$daughter)

```

Let us also check the densities and quartile plots of the heights the dataset: 

```{r}
#| layout-ncol: 2
# Set graph theme
theme_set(new = theme_custom())
#
Galton %>% group_by(sex) %>% 
  gf_density(~ height, group = ~ sex, 
             fill = ~ sex) %>% 
  gf_fitdistr(dist = "dnorm") %>% 
  gf_refine(scale_fill_brewer(palette = "Set1")) %>%
  gf_facet_grid(vars(sex)) %>% 
  gf_labs(title = "Facetted Density Plots")
##
Galton %>% group_by(sex) %>% 
  gf_qq(~ height, group = ~ sex, 
             colour = ~ sex, size = 0.5) %>% 
  gf_qqline(colour = "black") %>% 
  gf_refine(scale_color_brewer(palette = "Set1")) %>%
  gf_facet_grid(vars(sex)) %>% 
  gf_labs(title = "Facetted QQ Plots",
          x = "Theoretical quartiles",
          y = "Actual Data")

```

and the `father`'s heights:

```{r}
# Set graph theme
theme_set(new = theme_custom())
#
##
Galton %>% group_by(sex) %>% 
  gf_density(~ father, group = ~ sex, # no this is not weird
             fill = ~ sex) %>% 
  gf_fitdistr(dist = "dnorm") %>% 
  gf_refine(scale_fill_brewer(name = "Sex of Child", palette = "Set1")) %>%
  gf_facet_grid(vars(sex)) %>% 
  gf_labs(title = "Fathers: Facetted Density Plots",
                    subtitle = "By Sex of Child")

Galton %>% group_by(sex) %>% 
  gf_qq(~ father, group = ~ sex, # no this is not weird
             colour = ~ sex, size = 0.5) %>% 
  gf_qqline(colour = "black") %>% 
  gf_facet_grid(vars(sex)) %>% 
  gf_refine(scale_colour_brewer(name = "Sex of Child", palette = "Set1")) %>% 
  gf_labs(title = "Fathers Heights: Facetted QQ Plots",
          subtitle = "By Sex of Child",
          x = "Theoretical quartiles",
          y = "Actual Data")

```

The `shapiro.test` informs us that the child-related `height` variables are *not normally distributed*; though visually there seems nothing much to complain about. Hmmm...

[Dads are weird anyway](https://www.quora.com/My-Dad-acts-so-weird-is-it-normal), so we must not expect `father` heights to be normally distributed. 


### {{< iconify flat-color-icons workflow >}} Workflow: Inference

Let us now see how Correlation Tests can be performed  based on this dataset, to infer patterns in the population from which this dataset/sample was drawn.

We will go with classical tests first, and then set up a permutation test that does not need any assumptions. 

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}

#### Classical Tests
We perform the Pearson correlation test first: the data is **not normal** so we cannot really use this. We should use a *non-parametric* correlation test as well, using a `Spearman` correlation.

```{r}
#| label: corr-tests-sons
# Pearson (built-in test)
cor_son_pearson <- cor.test(son ~ father, 
                        method = "pearson", 
                        data = Galton_sons) %>% 
  broom::tidy() %>% 
  mutate(term = "Pearson Correlation r")
cor_son_pearson

cor_son_spearman <- cor.test(son ~ father, method = "spearman", data = Galton_sons) %>% 
  broom::tidy() %>% 
  mutate(term = "Spearman Correlation r")
cor_son_spearman

```

Both tests state that the correlation between `son` and `father` is significant. 

```{r}
#| label: corr-tests-daughters
# Pearson (built-in test)
cor_daughter_pearson <- cor.test(daughter ~ father, 
                        method = "pearson", 
                        data = Galton_daughters) %>% 
  broom::tidy() %>% 
  mutate(term = "Pearson Correlation r")
cor_daughter_pearson
##
cor_daughter_spearman <- cor.test(daughter ~ father, method = "spearman", data = Galton_daughters) %>% 
  broom::tidy() %>% 
  mutate(term = "Spearman Correlation r")
cor_daughter_spearman

```

Again both tests state that the correlation between `daughter` and `father` is significant. 

#### Intuitive
What is happening under the hood in `cor.test`?

To be Written Up! But when? 

#### Permutation Test

We can of course use a randomization based test for correlation. How would we mechanize this, what aspect would be randomize?

Correlation is calculated on a **vector-basis**: each individual observation of `variable#1` is multiplied by the **corresponding** observation of `variable#2`. Look at @eq-corr! So we might be able to randomize the *order* of this multiplication to see *how uncommon* this particular set of multiplications are. That would give us a `p-value` to decide if the **observed correlation** is close to the truth. So, onwards with our friend `mosaic`:

```{r}
obs_daughter_corr <- cor(Galton_daughters$father, Galton_daughters$daughter)
obs_daughter_corr

```

```{r}
corr_daughter_null <- do(4999) * cor.test(daughter ~ shuffle(father), 
                                          data = Galton_daughters) %>% 
  broom::tidy()
corr_daughter_null
corr_daughter_null %>% 
  gf_histogram(~ estimate, bins = 50) %>% 
  gf_vline(xintercept = obs_daughter_corr, 
           color = "red",linewidth = 1) %>% 
  gf_labs(title = "Permutation Null Distribution",
          subtitle = "Daughter Heights vs Father Heights")
##
p_value_null <- 2.0*mean(corr_daughter_null$estimate >= obs_daughter_corr)
p_value_null

```

We see that will all permutations of `father`, we are never able to hit the actual `obs_daughter_corr`! Hence there is a definite correlation between `father` height and `daughter` height.

#### The Linear Model

The premise here is that **many common statistical tests are special cases of the linear model**. A **linear model** estimates the relationship between dependent variable or  
"response" variable `height` and an explanatory variable or "predictor", `father`. It is assumed that the relationship is **linear**. $\beta_0$ is the *intercept* and $\beta_1$
is the slope of the linear fit, that **predicts** the value of `height` based the value of `father`.

$$
height = \beta_0 + \beta_1 \times father
$$
The model for Pearson Correlation tests is exactly the Linear Model:

$$
\begin{aligned}
height = \beta_0 + \beta_1 \times father\\
\\
H_0: Null\ Hypothesis\ => \beta_1 = 0\\\
H_a: Alternate\ Hypothesis\ => \beta_1 \ne 0\\
\end{aligned}
$$

Using the *linear model* method we get:

```{r}
# Linear Model
lin_son <- lm(son ~ father, data = Galton_sons) %>% 
  broom::tidy() %>% 
  mutate(term = c("beta_0", "beta_1")) %>% 
  select(term, estimate, p.value)
lin_son
##
lin_daughter <- lm(daughter ~ father, data = Galton_daughters) %>% 
  broom::tidy() %>% 
  mutate(term = c("beta_0", "beta_1")) %>% 
  select(term, estimate, p.value)
lin_daughter

```

Why are the respective $r$-s and $\beta_1$-s different, though the `p-value`-s is **suspiciously the same**!? Did we miss a factor of $\frac{sd(son/daughter)}{sd(father)} = ??$ somewhere...??

Let us **scale** the variables to within `{-1, +1}` : (subtract the mean and divide by sd) and re-do the Linear Model with **scaled** versions of `height` and `father`:

```{r}
# Scaled linear model
lin_scaled_galton_daughters <- lm(scale(daughter) ~ 1 + scale(father), data = Galton_daughters) %>% 
  broom::tidy() %>% 
  mutate(term = c("beta_0", "beta_1")) 
lin_scaled_galton_daughters 

```

Now you're talking!! The `estimate` is the same in both the classical test and the linear model! So we conclude:

1.  **When both target and predictor have the same standard deviation, the slope from the linear model and the Pearson correlation are the same**.

2.  There is this relationship between the **slope in the linear model** and **Pearson correlation**:

$$
Slope\ \beta_1 = \frac{sd_y}{sd_x} * r
$$

The slope is usually much more interpretable and informative than the correlation coefficient.

2.  Hence a linear model using `scale()` for both variables will show slope = **r**.

Slope_Scaled: `r lin_scaled_galton_daughters$estimate[2]` = Correlation: `r cor_daughter_pearson$estimate`

3.  Finally, the *p-value* for Pearson Correlation and that for the *slope* in the linear model is the same ($0.04280043$). Which means we cannot reject the NULL hypothesis of "no relationship" between `daughter`-s and `father`-s heights.

Can you complete this for the sons?

:::

:::{.hidden}
## {{< iconify pajamas issue-type-test-case >}} Case Study #2: Study and Grades


In some cases the **LINE** assumptions may not hold.

Nonlinear relationships, non-normally distributed data ( with large
**outliers** ) and working with *ordinal* rather than continuous data:
these situations necessitate the use of Spearman's *ranked* correlation
scores. (**Ranked**, not **sign-ranked**.).

See the example below: We choose to look at the `gpa_study_hours`
dataset. It has two numeric columns `gpa` and `study_hours`:

```{r gpa_study_hours}
glimpse(gpa_study_hours)

```

We can plot this:

```{r Pearson_example_3}

# Set graph theme
theme_set(new = theme_custom())
#
ggplot(gpa_study_hours, aes(x = study_hours, y = gpa)) + 
  geom_point() + 
  geom_smooth() + 
  labs(title = "GPA vs Study Hours",
       subtitle = "Pearson Correlation Test")

```

Hmm...not normally distributed, and there is a sort of increasing
relationship, however is it linear? And there is some evidence of
heteroscedasticity, so the LINE assumptions are clearly in violation.
Pearson correlation would not be the best idea here.

Let us quickly try it anyway, using a Linear Model for the **scaled**
`gpa` and `study_hours` variables, from where we get:

```{r Pearson_example_1}
# Pearson Correlation as Linear Model
model_gpa <-
  lm(scale(gpa) ~ 1 + scale(study_hours), data = gpa_study_hours)
##
model_gpa %>%
  broom::tidy() %>% 
  mutate(term = c("beta_0", "beta_1")) %>% 
  cbind(confint(model_gpa) %>%                                              as_tibble()) %>%  
  select(term, estimate, p.value, `2.5 %`, `97.5 %`)

```

The correlation estimate is $0.133$; the `p-value` is $0.065$ (and the
`confidence interval` includes $0$).

Hence we fail to reject the NULL hypothesis that `study_hours` and `gpa`
have no relationship. But can this be right?

Should we use another test, that does not **need** the LINE assumptions?

#### "Signed Rank" Values

Most statistical tests use the **actual values** of the data variables.
However, in some *non-parametric* statistical tests, the data are used
in **rank-transformed** sense/order. (In some cases the **signed-rank**
of the data values is used instead of the data itself.)

**Signed Rank** is calculated as follows:

1.  Take the absolute value of each observation in a sample

2.  Place the <u>*ranks*</u> in order of (absolute magnitude). The
    smallest number has *rank = 1* and so on. This gives is **ranked
    data**.

3.  Give each of the ranks the sign of the original observation ( + or
    -). This gives us **signed** ranked data.

```{r signed_rank_function}
signed_rank <- function(x) {sign(x) * rank(abs(x))}

```

#### Plotting Original and Signed Rank Data

Let us see how this might work by comparing data and its signed-rank
version...A quick set of plots:

```{r data_plots}
#| include: false
#| eval: false
# Set graph theme
theme_set(new = theme_custom())
#

p1 <- ggplot(mydata_long, aes(x = group, y = value)) +
  geom_jitter(width = 0.02,
              height = 0,
              aes(colour = group),
              size = 4) +
  geom_segment(data = mydata_wide,
               aes(
                 y = 0,
                 yend = 0,
                 x = .75,
                 xend = 1.25
               )) +
  geom_text(aes(x = 1, y = 0.5, label = "0")) +
  geom_segment(data = mydata_wide,
               aes(
                 y = 0.3,
                 yend = 0.3,
                 x = 1.75 ,
                 xend = 2.25
               )) +
  geom_text(aes(x = 2, y = 0.6, label = "0.3")) +
  geom_segment(data = mydata_wide,
               aes(
                 y = 0.5,
                 yend = 0.5,
                 x = 2.75,
                 xend = 3.25
               )) +
  geom_text(aes(x = 3, y = 0.8, label = "0.5")) +
  labs(title = "Original Data",
       subtitle = "Black Lines show Means") +
  ylab("Response Variable")

p2 <- mydata_long %>%
  group_by(group) %>%
  mutate(s_value = signed_rank(value)) %>%
  ggplot(., aes(x = group, y = s_value)) +
  geom_jitter(width = 0.02,
              height = 0,
              aes(colour = group),
              size = 4) +
  stat_summary(
    fun = "mean",
    geom = "point",
    colour = "red",
    size = 8
  ) +
  labs(title = "Signed Rank of Data",
       subtitle = "Red Points are means of Ranks!") +
  ylab("Signed Rank of Response Variable")

patchwork::wrap_plots(p1, p2, nrow = 1, guides = "collect") 

```

So the means of the **ranks** three separate variables seem to be in the
same order as the means of the data variables themselves.

How about associations between data? Do ranks reflect well what the data
might?

```{r Spearman_Plot}
#| eval: false
#| include: false
# Plot the data

# Set graph theme
theme_set(new = theme_custom())
#

p1 <- ggplot(mydata_wide, aes(x, y1)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle(" Pearson Correlation\n and Linear Models")

# Plot ranked data
p2 <- mydata_wide %>% 
  mutate(x_rank = rank(x),
         y_rank = rank(y1)) %>%
  ggplot(.,aes(x_rank, y_rank)) + 
  geom_point(shape = 15, size = 2) +
  geom_smooth(method = "lm") + 
  ggtitle(" Spearman Ranked Correlation\n and Linear Models")

patchwork::wrap_plots(p1,p2, nrow = 1, guides = "collect")

```

The slopes are almost identical, $0.25$ for both original data and
ranked data for $y1\sim x$. So maybe *ranked* and even *sign_ranked*
data could work, and if it can work despite LINE assumptions not being
satisfied, that would be nice!

#### How does Sign-Rank data work?

TBD: need to add some explanation here.

Spearman correlation = Pearson correlation using the rank of the data
observations. Let's check how this holds for a our x and y1 data:

So the Linear Model for the Ranked Data would be:

$$
\begin{aligned}
y = \beta_0 + \beta_1 \times rank(x)\\
\\
H_0: Null\ Hypothesis\ => \beta_1 = 0\\\
H_a: Alternate\ Hypothesis\ => \beta_1 \ne 0\\
\end{aligned}
$$

#### Code

```{r}
#| include: false
#| eval: false
# Spearman
cor1 <- cor.test(x, y1, method = "spearman") %>%
  broom::tidy() %>% mutate(term = "Spearman Correlation ") %>% select(term, estimate, p.value)
cor1

```

```{r}
#| include: false
#| eval: false
# Pearson using ranks
cor2 <- cor.test(rank(y1), rank(x), method = "pearson") %>%
  broom::tidy() %>% select(estimate, p.value)
cor2
```

```{r}
#| include: false
#| eval: false
# Linear Models using rank
cor3 <- lm(rank(y1) ~ 1 + rank(x),data = mydata_wide) %>% 
  broom::tidy() %>% select(estimate, p.value)
cor3
```

Notes:

1.  When ranks are used, the slope of the linear model ($\beta_1$) has
    the same value as the Spearman correlation coefficient ( $\rho$ ).

2.  Note that the slope from the linear model now has an intuitive
    interpretation: **the number of ranks y changes for each change in
    rank of x**. ( Ranks are "independent" of `sd` )

#### Example

We examine the `cars93` data, where the numeric variables of interest
are `weight` and `price`.

```{r Spearman_example_1}

# Set graph theme
theme_set(new = theme_custom())
#

cars93 %>% 
  ggplot(aes(weight, price)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, lty = 2) + 
  labs(title = "Car Weight and Car Price have a nonlinear relationship") + theme_classic()
```

Let us try a Spearman Correlation score for these variables, since the
data are not linearly related and the variance of `price` also is not
constant over `weight`

```{r Spearman_example_2}
#| warning: false
#| message: false

# Set graph theme
theme_set(new = theme_custom())
#

cor.test(cars93$price, cars93$weight, method = "spearman") %>% broom::tidy()

# Using linear Model
lm(rank(price) ~ rank(weight), data = cars93) %>% summary()

# Stats Plot
ggstatsplot::ggscatterstats(data = cars93, x = weight, 
                            y = price,
                            type = "nonparametric",
                            title = "Cars93: Weight vs Price",
                            subtitle = "Spearman Correlation")
```

We see that using ranks of the `price` variable, we obtain a Spearman's
$\rho = 0.882$ with a `p-value` that is very small. Hence we are able to
reject the NULL hypothesis and state that there is a relationship
between these two variables. The **linear** relationship is evaluated as
a correlation of `0.882`.

```{r}
# Other ways using other packages
mosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours) %>%
  broom:: tidy() %>% 
  select(estimate, p.value, conf.low, conf.high)

```

```{r}

statsExpressions::corr_test(data = gpa_study_hours, 
                            x = study_hours, 
                            y = gpa)
```
:::

## {{< iconify mingcute thought-line >}} Wait, But Why?

Correlation tests are useful to understand the relationship between two variables, but they do not imply causation. A high correlation does not mean that one variable causes the other to change. It is essential to consider the context and other factors that may influence the relationship.

## {{< iconify fluent-mdl2 decision-solid >}} Conclusion

Correlation tests are a powerful way to understand the relationship between two variables. They can be performed using classical methods like Pearson and Spearman correlation, or using more robust methods like permutation tests. The linear model approach provides a deeper understanding of the relationship, especially when the assumptions of normality and homoscedasticity are met.

## {{< iconify openmoji person >}} Your Turn

1. Try the datasets in the `infer` package. Use `data(package = "infer")` in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. `babynames`) to read it. 

2. Same with the `resampledata` and `resampledata3` packages. 


## {{< iconify ooui references-rtl >}} References

1. *Common statistical tests are linear models (or: how to teach
    stats)* by [Jonas Kristoffer Lindeløv](https://lindeloev.github.io/tests-as-linear/)\
    [CheatSheet](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf)\
1.    *Common statistical tests are linear models: a work through* by [Steve Doogue](https://steverxd.github.io/Stat_tests/)\
1.    [Jeffrey Walker "Elements of Statistical Modeling for Experimental Biology"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/)\
1.   Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: [OpenIntro Statistics](https://www.openintro.org/book/os/)\
1.   Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling by [Måns
    Thulin](http://www.modernstatisticswithr.com/)\
1.    [Jeffrey Walker "A
    linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables)
    
    

::: {#refs style="font-size: 60%;"}

```{r}
#| echo: false
# scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("easystats", "ggExtra", "ggstatsplot",
           "openintro", "resampledata3", "statsExpressions")
) %>%
  knitr::kable(format = "simple")

```

:::


