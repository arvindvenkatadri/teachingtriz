---
title: "Modelling with Linear Regression"
author: "Arvind Venkatadri"
date: 13/Apr/2023
date-modified: "`r Sys.Date()`"
order: 20
image: featured.jpg
image-alt: ""
categories: 
  - Linear Regression
  - Quantitative Predictor
  - Quantitative Response
  - Sum of Squares
  - Residuals
abstract: "Using Regression to predict Quantitative Target Variables"
---

## {{< fa folder-open >}} Slides and Tutorials {#sec-tutorials}

|                                                                                                                                       |                                                                                                                                         |                                                                                                             |     |
|---------------------|---------------------|----------------|--------------|
| <a href="./files/forward-selection-1.qmd"><i class="fa-brands        fa-r-project"></i> Multiple Regression - Forward Selection</a>   | <a href="./files/backward-selection-1.qmd"><i class="fa-brands        fa-r-project"></i> Multiple Regression - Backward Selection</a>   | <a href="./files/lin-perm.qmd"> <i class="fa-brands fa-r-project"></i> Permutation Test for Regression</a>  |     |

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
#| message: false
#| warning: false
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
#options(scipen = 1, digits = 3) #set digits to three decimal places
library(tidyverse)
library(ggformula)
library(mosaic)
library(GGally)
library(corrplot)
library(corrgram)

library(nomnoml)

```

```{r}
#| label: plot theme
# Let us set a plot theme for Data visualization

my_theme <- function(){  # Creating a function
  theme_classic() +  # Using pre-defined theme as base
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(size = 10, face = "bold"),  
        # Customizing axes text      
        axis.text.y = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 12, face = "bold"),  
        # Customizing axis title
        panel.grid = element_blank(),  # Taking off the default grid
        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , "cm"),
        legend.text = element_text(size = 8, face = "italic"),  
        # Customizing legend text
        legend.title = element_text(size = 10, face = "bold"),  
        # Customizing legend title
        legend.position = "right",  # Customizing legend position
        plot.caption = element_text(size = 8))  # Customizing plot caption
}   

```

## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

One of the most common problems in Prediction Analytics is that of
predicting a Quantitative response variable, based on one or more
Quantitative predictor variables or *features*. This is called Linear
Regression. We will use the intuitions built up during our study of
ANOVA to develop our ideas about Linear Regression.

Suppose we have data on salaries in a Company, with years of study and
previous experience. Would we be able to predict the prospective salary
of a new candidate, based on their years of study and experience? Or
based on mileage done, could we predict the resale price of a used car?
These are typical problems in Linear Regression.

In this tutorial, we will use the Boston housing dataset. Our research
question is:

::: callout-note
## Research Question

How do we predict the price of a house in Boston, based on other
parameters Quantitative parameters such as area, location, rooms, and
crime-rate in the neighbourhood?
:::

## {{< iconify radix-icons box-model >}} The Linear ( Regression ) Model

The premise here is that many common statistical tests are special cases
of the linear model.

A linear model estimates the relationship between one *continuous* or
*ordinal* variable (dependent variable or "response") and one or more
other variables (explanatory variable or "predictors"). It is assumed
that the relationship is linear:[^1]

$$
y = \beta_0 + \beta_1 *x
$$

$\beta_0$ is the *intercept* and $\beta_1$ is the slope of the linear
fit, that **predicts** the value of y based the value of x. Each
prediction leaves a small "residual" error between the actual and
predicted values. $\beta_0$ and $\beta_1$ are calculated based on
minimizing the *sum of square*s of these residuals, and hence this
method is called "ordinary least squares" (**OLS**) regression.

![Least Squares](../../../../../materials/images/OLS.png){height="360"}

The net *area* of all the shaded squares is minimized in the calculation
of $\beta_0$ and $\beta_1$. It is also possible that there is more than
one explanatory variable: this is **multiple regression.**

$$
y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 ...+ \beta_n*x_n
$$

where each of the $\beta_i$ are slopes defining the relationship between
y and $x_i$. Together, the RHS of that equation defines an n-dimensional
*hyperplane*.
The model is **linear in the parameters** $\beta_i$, e.g. these are OK:

$$
\color{blue}{
\begin{cases}
 & y_i = \pmb\beta_0 + \pmb\beta_1x_1 + \pmb\beta_2x_1^2 + \epsilon_i\\
 & y_1 = \pmb\beta_0 + \pmb\gamma_1\pmb\delta_1x_1 + exp(\pmb\beta_2)x_2+ \epsilon_i\\
\end{cases}
}
$$
but not, for example, these:

$$
\color{red}{
\begin{cases}
 & y_i = \pmb\beta_0 + \pmb\beta_1x_1^{\beta_2} + \epsilon_i\\
 & y_i = \pmb\beta_0 + exp(\pmb\beta_1x_1) + \epsilon_i\\
\end{cases}
}
$$

As per Lindoloev, many statistical tests, going from one-sample
`t-tests` to `two-way ANOVA`, are special cases of this system. Also see
[Jeffrey Walker "A
linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables)

## {{< iconify simple-icons hypothesis >}} Linear Models as Hypothesis Tests

Using linear models is based on the idea of **Testing of Hypotheses**.
The Hypothesis Testing method typically defines a NULL Hypothesis where
the statements read as "**there is no relationship**" between the
variables at hand, explanatory and responses. The Alternative Hypothesis
typically states that there *is* a relationship between the variables.

Accordingly, in fitting a linear model, we follow the process as
follows: With $y = \beta_0 + \beta_1 *x$

1.  Make the following hypotheses: $$
    NULL\ Hypothesis\ H_0 => x\ and\ y\ are\ unrelated.\ (\beta_1 = 0)
    $$ $$
    Alternate\ Hypothesis\ H_1 => x\ and\ y\ are\ linearly\ related\ (\beta_1 \ne 0)
    $$
2.  We "assume" that $H_0$ is true.
3.  We calculate $\beta_1$.
4.  We then find probability **p** that $\beta_1 = Estimated\ Value$
    **when the NULL Hypothesis** is **assumed** TRUE. This is the
    **p-value**. If that probability is **p\>=0.05**, we say we "cannot
    reject" $H_0$ and there is unlikely to be significant linear
    relationship.
5.  However, if **p\<= 0.05** can we reject the NULL hypothesis, and say
    that there could be a significant linear relationship, because the
    probability **p** that $\beta_1 = Estimated\ Value$ by mere chance
    under $H_0$ is very small.

## {{< iconify icon-park-outline thinking-problem >}} Assumptions in Linear Models {#sec-assumptions-in-linear-models}

We can write the assumptions in Linear Regression Models as an acronym,
**LINE**:\
1. **L**: $\color{blue}{linear}$ relationship\
2. **I**: Errors are **independent** (across observations)\
3. **N**: y is $\color{red}{normally}$ distributed at each "level" of
x.\
4. **E**: equal variance at all levels of x. No *heteroscedasticity*.\

![OLS
Assumptions](../../../../../materials/images/ols_assumptions.png){height="360"}

Hence a very concise way of expressing the Linear Model is:

$$
y \sim N(x_i^T * \beta, ~~\sigma^2)
$$

::: callout-important
## General Linear Models

The target variable $y$ is modelled as a *normally* distributed variable
**whose mean depends upon a linear combination of predictor variables**
$x$, and whose variance is $\sigma^2$.
:::

Let us now read in the data and check for these assumptions as part of
our Workflow.

## {{< iconify flat-color-icons workflow >}} Workflow: Read the Data

```{r}

data("BostonHousing2", package = "mlbench")
housing <- BostonHousing2
inspect(housing)

```

The original data are 506 observations on 14 variables, `medv` being the
target variable:

|         |                                                                       |
|:-------------|:---------------------------------------------------------|
| crim    | per capita crime rate by town                                         |
| zn      | proportion of residential land zoned for lots over 25,000 sq.ft       |
| indus   | proportion of non-retail business acres per town                      |
| chas    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| nox     | nitric oxides concentration (parts per 10 million)                    |
| rm      | average number of rooms per dwelling                                  |
| age     | proportion of owner-occupied units built prior to 1940                |
| dis     | weighted distances to five Boston employment centres                  |
| rad     | index of accessibility to radial highways                             |
| tax     | full-value property-tax rate per USD 10,000                           |
| ptratio | pupil-teacher ratio by town                                           |
| b       | $1000(B - 0.63)^2$ where B is the proportion of Blacks by town        |
| lstat   | percentage of lower status of the population                          |
| medv    | median value of owner-occupied homes in USD 1000's                    |

The corrected data set has the following additional columns:

|       |                                                              |
|:------|:-------------------------------------------------------------|
| cmedv | corrected median value of owner-occupied homes in USD 1000's |
| town  | name of town                                                 |
| tract | census tract                                                 |
| lon   | longitude of census tract                                    |
| lat   | latitude of census tract                                     |

Our response variable is `cmedv`, the *corrected median value of
owner-occupied homes in USD 1000'*s. Their are many Quantitative feature
variables that we can use to predict `cmedv`. And there are two
Qualitative features, `chas` and `tax`.

## {{< iconify flat-color-icons workflow >}} Workflow: EDA

In order to fit the linear model, we need to choose **predictor**
variables that have strong correlations with the **target** variable. We
will first do this with `GGally`, and then with the `tidyverse` itself.
Both give us a very unique view into the correlations that exist within
this dataset.

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
### Correlations with GGally

Let us select a few sets of Quantitative and Qualitative features, along
with the target variable `cmedv` and do a pairs-plots with them:

```{r}
#| label: pairs plots 1
#| message: false
#| warning: false
#| layout-nrow: 2

theme_set(theme_bw())
housing %>%
  # Target variable cmedv
  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access
  select(cmedv, rm, age, dis) %>%
  GGally::ggpairs(title = "Plot 1",
                  lower = list(continuous = wrap("smooth", alpha = 0.2)))

housing %>%
  # Target variable cmedv
  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000
  select(cmedv, rad, zn, indus, tax) %>%
  GGally::ggpairs(title = "Plot 2", 
                  lower = list(continuous = wrap("smooth", alpha = 0.2)))

housing %>%
  # Target variable cmedv
  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population
  select(cmedv, crim, nox, rad, b, lstat) %>%
  GGally::ggpairs(title = "Plot 3", 
                  lower = list(continuous = wrap("smooth", alpha = 0.2)))


```

Clearly, `rm` (avg. number of rooms) is a big determining feature for
median price `cmedv`. This we infer based on the large magnitudes of
correlations of `rm` with`cmedv`. The variable`age` (proportion of
owner-occupied units built prior to 1940) may also be a significant
influence on `cmedv`. (See top row)

None of the Quant variables `rad, zn, indus, tax` have a overly strong
correlation with `cmedv`. (See top row)

The variable `lstat` (proportion of lower classes in the neighbourhood)
as expected, has a strong (negative) correlation with `cmedv`;
`rad`(index of accessibility to radial highways), `nox`(nitrous oxide)
and `crim`(crime rate) also have fairly large correlations with `cmedv`.

::: callout-important
## Correlation Scores and Uncertainty

Recall that `cor.test` reports a correlation score and the `p-value` for
the same. There is also a `confidence interval` reported for the
correlation score, an interval within which we are 95% sure that the
true correlation value is to be found. Note that `GGally` too reports
the significance of the correlation scores using stars, `***` or `**`.
This indicates the p-value in the scores obtained by `GGally`;
Presumably, there is an internal `cor.test` that is run for each pair of
variables and the p-value and confidence levels are also computed
internally.
:::

Let us plot (again) scatter plots of Quant Variables that have strong
correlation with `cmedv`:

```{r}
#| layout-ncol: 2
gf_point(
  data = housing,
  cmedv ~ age,
  title = "Price vs Proportion of houses older than 1940",
  ylab = "Median Price",
  xlab = "Proportion of older-than-1940 buildings"
) %>%
  gf_theme(my_theme())

gf_point(
  data = housing,
  cmedv ~ lstat,
  title = "Price vs Proportion of lower classes in the neighbourhood",
  ylab = "Median Price",
  xlab = "proportion of lower classes in the neighbourhood"
) %>%
  gf_theme(my_theme())

gf_point(
  data = housing,
  cmedv ~ rm,
  title = "Price vs Average no. of Rooms",
  ylab = "(cmedv) Median Price",
  xlab = "(rm) Avg. No. of Rooms"
) %>%
  gf_theme(my_theme())


```

So, `rm` does have a positive effect on `cmedv`, and `age` may have a
(mild?) negative effect on `cmedv`; `lstat` seems to have a pronouced
negative effet on `cmedv`. We have now managed to get a decent idea
which *Quant predictor variables* might be useful in modelling `cmedv`:
`rm`, `lstat` for starters, then perhaps`age`.

Let us also check the *Qualitative predictor variables*: Access to the
Charles river (`chas`) does seem to affect the prices somewhat.

```{r}
#| label: Charles River
theme_set(theme_bw())
housing %>%
  # Target variable cmedv
  # Predictor Access to Charles River
  select(cmedv, chas) %>%
  GGally::ggpairs(title = "Plot 4", 
                  lower = list(continuous = wrap("smooth", 
                                                 alpha = 0.2)))

```

Look at the bar plot above. While not too many properties can be near
the Charles River (for obvious reasons) the box plots do seem to show
some dependency of `cmedv` on `chas`.

::: callout-note
Qualitative predictors for a Quantitative target can be included in the
model using what is called *dummy variables*, where each *level* of the
Qualitative variable is given a **one-hot** kind of encoding. See for
example <https://www.statology.org/dummy-variables-regression/>
:::

### Correlations using cor.test and purrr

This is somewhat advanced material: We will use the `purrr` package to
develop all correlations with respect to our target variable in one shot
and also plot these correlation test scores in an error-bar plot. This
has the advantage of being able to depict all correlations in one plot.
(We will use this approach again here when we trim our linear models
down from the *maximal* one to a workable one of lesser complexity.).
Let us do this.

We develop a *list object* containing all correlation test results with
respect to `cmedv`, tidy these up using `broom::tidy`, and then plot
these:

```{r}
#| label: corrtest plots
all_corrs <- housing %>% 
  select(where(is.numeric)) %>% 
  # leave off target variable cmedv and IDs
  # get all the remaining ones
  select(-cmedv, -medv) %>%  

  purrr::map(.x = ., # All numeric variables selected in the previous step
             .f = \(.x) cor.test(.x, housing$cmedv)) %>% # Apply the cor.test with `cmedv`
  
  # Tidy up the cor.test outputs into neat columns
  # Need ".id" column to keep track of predictor variable name
  map_dfr(broom::tidy, .id = "predictor") 

all_corrs

all_corrs %>%
  gf_hline(
    yintercept = 0,
    color = "grey",
    linewidth = 2,
    title = "Correlations: Target Variable vs All Predictors",
    subtitle = "Boston Housing Dataset"
  ) %>%
  gf_errorbar(
    conf.high + conf.low ~ reorder(predictor, estimate),
    colour = ~ estimate,
    width = 0.5,
    linewidth = ~ -log10(p.value),
    caption = "Significance = -log10(p.value)"
  ) %>%
  
  # Plot points(smallest geom) last!
  gf_point(estimate ~ reorder(predictor, estimate)) %>%
  gf_labs(x = "Predictors", y = "Correlation with cmedv") %>%
  
  gf_theme(my_theme()) %>%
  
  # tilt the x-axis labels for readability
  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %>%
  
  # Colour and linewidth scales + legends
  gf_refine(
    scale_colour_distiller("Correlation", type = "div", palette = "RdBu"),
    scale_linewidth_continuous("Significance", range = c(0.25, 3), 
                               
  # guide_legend(reverse = TRUE): Fat Lines mean higher significance
    )) %>%
  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))

```

We can clearly see that `rm` and `lstat` have strong correlations with
`cmedv` and should make good choices for setting up a minimal linear
regression model. (`medv` is the older errorred version of `cmedv`)
:::

::: callout-note
## Simple Regression vs Multiple Regression

If there are many predictor variables, we would typically want to use
more of them to make our model and predictions. There are three ways[^2]
to include more predictors:

-   **Backward Selection**: We would typically start with a **maximal
    model**[^3] and progressively simplify the model by knocking off
    predictors that have the least impact on model accuracy.
-   **Forward Selection**: Start with no predictors and systematically
    add them one by one to increase the quality of the model
-   **Mixed Selection**: Wherein we start with no predictors and add
    them to gain improvement, or remove them at as their *significance*
    changes based on other predictors that have been added.

We will do the first two in the other tutorials (see @sec-tutorials
above); Mixed Selection we will leave for a more advanced course. But
for now we will first use just one predictor `rm`(Avg. no. of Rooms) to
model housing prices.
:::

## {{< iconify flat-color-icons workflow >}} Workflow: Model Building

We will first execute the `lm` test with code and evaluate the results.
Then we will do an intuitive walk through of the process and finally,
hand-calculate entire analysis for clear understanding.

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
### {{< iconify mingcute code-fill >}} Model Code

R offers a very simple command `lm` to execute an Linear Model: Note the
familiar `formula` of stating the variables: ( $y \sim x$; where $y$ =
target, $x$ = predictor)

```{r}
#| label: Linear Model with code
housing_lm <- lm(cmedv ~ rm, data = housing)
summary(housing_lm)

```

The model for $\widehat{cmedv}$ , the prediction for `cmedv`can be
written in the form of $y = mx + c$, as:

$$
\widehat{cmedv} \sim -34.65924 + 9.09967* rm
$$ {#eq-rm-model}

::: callout-important
-   The **effect size** of `rm` on predicting `cmedv` a (slope) value of
    $9.09967$ which is significant at p-value of $<2.2e-16$; for every
    one room increase in `rm`, we have a \$ USD 90997\$ increase in
    median price `cmedv`.
-   The **F-statistic** for the Linear Model is given by $474.3$, which
    is very high. (We will use the F-statistic again when we do Multiple
    Regression.)
-   The `R-squared` value is 48% which means that `rm` is able to
    explain about half of the trend in `cmedv`; there is substantial
    variation in `cmedv` that is left to explain, an indication that we
    should perhaps use a richer model, with more predictors. We will
    explore this in the Tutorials. @sec-tutorials
:::

We can plot the scatter plot of these two variables with the model also
over-plotted.

```{r}
#| label: model
#| layout-ncol: 3
#| fig-width: 5
#| fig-height: 4
# Tidy Data frame for the model using `broom`
housing_lm_tidy <- 
  housing_lm %>% 
  broom::tidy(conf.int= TRUE, 
              conf.level = 0.95)
housing_lm_tidy

housing_lm_augment <- 
  housing_lm %>% 
  broom::augment(se_fit = TRUE,
                 interval = "confidence")
housing_lm_augment

intercept <- 
  housing_lm_tidy %>%
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  as.numeric()

slope <- 
  housing_lm_tidy %>%
  filter(term == "rm") %>%
  select(estimate) %>%
  as.numeric()

gf_point(
  data = housing,
  cmedv ~ rm,
  title = "Price vs Average no. of Rooms",
  ylab = "Median Price",
  xlab = "Avg. No. of Rooms",
  alpha = 0.2
) %>%
  
  # Plot the model equation
  gf_abline(slope = slope, intercept = intercept, colour = "lightcoral",
            linewidth = 2)  %>%
  
  # Plot the model prediction points on the line
  gf_smooth(method = "lm", geom = "point", color = "yellow", size = 0.5) %>%
  
  gf_segment(
    0 + 29 ~ 7 + 7, # manually calculated
    linetype = "dashed",
    color = "dodgerblue",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "last",
      type = "closed"
    ),
    data = housing_lm_augment
  ) %>%
  
  gf_segment(
    29 + 29 ~ 2.5 + 7, # manually calculated
    linetype = "dashed",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "first",
      type = "closed"
    ),
    color = "dodgerblue",
    data = housing_lm_augment
  ) %>%
  
  gf_refine(
    scale_x_continuous(limits = c(2.5, 10),
                       expand = c(0, 0)),
    # removes plot panel margins
    scale_y_continuous(limits = c(0, 55),
                       expand = c(0, 0))
  )  %>% 
  gf_theme(my_theme())

```

For any new value of `rm`, we go up to the vertical blue line and read
off the predicted median price by following the horizontal blue line.
That is how the model is used (by hand).

In practice, we use the `broom` package functions (`tidy`, `glance` and
`augment`) to obtain a clear view of the model parameters and
predictions of `cmedv` for all *existing* values of `rm`. We see
estimates for the intercept and slope (`rm`) for the linear model, along
with the *standard errors* and *p.values* for these estimated
parameters. And we see the fitted values of `cmedv` for the existing
`rm`; these values will naturally lie **on** the straight-line depicting
the model. We will examine this `augment`-ed data more in
@sec-diagnostics.

To predict `cmedv` with *new* values of `rm`, we use `predict`. Let us
now try to make predictions with some new data:

```{r}
#| label: predictions with new data
new <- tibble(rm = seq(3, 10)) # must be named "rm"
new %>% mutate(predictions =
                 stats::predict(
                   object = housing_lm,
                   newdata = .,
                   se.fit = FALSE
                 ))
```

Note that "negative values" for predicted `cmedv` would have no meaning!

### {{< iconify mdi thinking >}} Linear Model Intuitive {#sec-lm-intuitive}

All that is very well, but what is happening under the hood of the `lm`
command? Consider the `cmedv` (target) variable and the `rm`
feature/predictor variable. What we do is:

1.  Plot a scatter plot `gf_point(cmedv ~ rm, housing)`
2.  Find a line that, in some way, gives us some prediction of `cmedv`
    for any given `rm`
3.  Calculate the errors in prediction and use those to find the "best"
    line.
4.  Use that "best" line henceforth as a model for prediction.

How does one fit the "best" line? Consider a choice of "lines" that we
can use to fit to the data. Here are 6 lines of varying slopes (and
intercepts ) that we can try as candidates for the best fit line:

```{r}
#| echo: false
# This code is not to be dissected in class
# For exposition purposes only
# Discuss the results only (graphs)

set.seed(1234)
housing_sample <- housing_lm_augment %>% 
  slice_sample(n = 15)
mean_cmedv_sample <-
  mean( ~ cmedv, na.rm = TRUE, data = housing_sample)
mean_rm_sample <- mean( ~ rm, na.rm = TRUE, data = housing_sample)

lm_sample <- tibble(
  slope = slope + c(5, 2, 0, -2, -5, -slope),
  intercept = intercept + c(-30, -15, 0, 10, 30, -intercept + mean_cmedv_sample),
  # List column containing `housing_sample`
  # No repetition needed !!!
  # Auto recycle for each of slope + intercept
  sample = list(housing_sample)
)

lm_sample <- lm_sample %>% 
  mutate(line = pmap(
    .l = list(intercept, slope, sample),
    .f = \(intercept, slope, sample) 
            tibble(pred = intercept + slope * sample$rm, 
                   rm = sample$rm))) %>%
  
  mutate(graphs = pmap(
    .l = list(sample, line),
    .f = \(sample, line)
    gf_point(cmedv ~ rm, data = sample, size = 2) %>%
      gf_line(
        pred ~ rm,
        data = line ,
        color = "dodgerblue",
        linewidth = 2
      ) %>%
      gf_refine(
        scale_x_continuous(limits = c(2.5, 10),
                           expand = c(0, 0)),
        # removes plot panel margins
        scale_y_continuous(limits = c(0, 55),
                           expand = c(0, 0))
      ) %>% gf_theme(my_theme())))  

```

```{r}
#| echo: false
#| layout-ncol: 3
lm_sample %>% pluck("graphs",1)
lm_sample %>% pluck("graphs",2)
lm_sample %>% pluck("graphs",3)
lm_sample %>% pluck("graphs",4)
lm_sample %>% pluck("graphs",5)
lm_sample %>% pluck("graphs",6)


```

It should be apparent that while we cannot determine which line may be
the best, the **worst** line seems to be the one in the final plot,
which ignores the x-variable `rm` altogether. This corresponds to the
*NULL Hypothesis*, that there is *no relationship* between the two
variables. Any of the other lines could be a decent candidate, so how do
we decide?

```{r}
#| echo: false
#| layout-ncol: 2
#| warning: false
#| 
housing_sample %>% 
  gf_hline(yintercept =  ~ mean_cmedv_sample,
           color = "dodgerblue", linewidth = 2) %>%
  gf_segment(
    data = housing_sample,
    color = "springgreen3",
    mean_cmedv_sample + cmedv ~ rm + rm,
    title = "Fig A: Price vs \nAverage no. of Rooms",
    subtitle = "NULL Hypothesis",
    ylab = "cmedv (Median Price)",
    xlab = "rm (Avg. No. of Rooms)"
  ) %>%
  gf_point(cmedv ~ rm) %>%
  gf_text(
    mean_cmedv_sample - 2 ~ 7.5,
    label = expression(paste(mu, "_tot")),
    inherit = F,
    family = "Merri"
  ) %>%
  gf_theme(my_theme())

housing_sample  %>%
  gf_segment(
    .fitted + cmedv ~ rm + rm,
    color = "orangered1",
    title = "Fig B: Price vs \nAverage no. of Rooms",
    subtitle = "Alternative Hypothesis",
    ylab = "cmedv (Median Price)",
    xlab = "rm (Avg. No. of Rooms)"
  ) %>%
  gf_abline(slope = slope,
            intercept = intercept,
            colour = "dodgerblue", linewidth = 2) %>%
  gf_point(cmedv ~ rm) %>%
  gf_theme(my_theme())


```

In Fig A, the *horizontal* [blue line]{style="color: dodgerblue;"} is
the overall mean of `cmedv`, denoted as $\mu_{tot}$. The vertical [green
lines]{style="color: palegreen;"} to the points show the departures of
each point from this overall mean, called
[**residuals**]{style="background-color: yellow;"}. The sum of *squares*
of these residuals in Fig A is called the [**Total Sum of Squares**
(SST)]{style="background-color: yellow;"}.

$$
SST = \Sigma (y - \mu_{tot})^2
$$ {#eq-SST}

In Fig B, the vertical [red lines]{style="color: red;"} are the
residuals of each point from the potential line of fit. The sum of the
*squares* of these lines is called the [**Total Error Sum of Squares**
(SSE)]{style="background-color: yellow;"}.

$$
SSE = \Sigma [(y - a - b * rm)^2]
$$ {#eq-SSE}

It should be apparent that if there is any positive linear relationship
between `cmedv` and `rm`,then $SSE < SST$.

How do we get the optimum slope + intercept? If we plot the $SSE$ as a
function of varying slope, we get:

```{r}
#| label: slopes vs SSE
#| echo: false
sim_model <- tibble(b = slope + seq(-5,5),
                  a = intercept,
                  dat = list(tibble(cmedv = housing_sample$cmedv, 
                               rm = housing_sample$rm))) %>% 
  mutate(r_squared= pmap_dbl(
    .l = list(a,b,dat),
    .f = \(a,b, dat) sum((dat$cmedv - (b*dat$rm + a))^2))) 
min_r_squared <- sim_model %>% select(r_squared) %>% min()
min_slope <- sim_model %>% filter(r_squared == min_r_squared) %>% select(b) %>% as.numeric()
sim_model %>% 
  gf_point(r_squared ~ b,data = ., size = 2) %>% 
  gf_line(ylab = "SSE", xlab = "slope",title = "Error vs Slope") %>% 
  gf_hline(yintercept = min_r_squared, color = "red") %>%
  gf_segment(min_r_squared + 0 ~ min_slope + min_slope, 
             colour = "red", 
             arrow = arrow(ends = "last", length = unit(1, "mm")))  %>% 
  gf_theme(my_theme()) %>%
  gf_refine(coord_cartesian(expand = FALSE), 
            expand_limits(y = c(0, 20000), x = c(3.5, 15)))

```

We see that there is a quadratic minimum $SSE$ at the optimum value of
slope and at all other slopes, the $SSE$ is higher. We can use this to
find the optimum slope, which is what the function `lm` does.

### {{< iconify material-symbols slideshow-sharp >}} Linear Models Manually Demonstrated (Apologies to Spinoza)

Let us hand-calculate the numbers so we know what the test is doing.
Here is the SST: we pretend that there is no relationship between
`cmedv` ans `rm` and compute a **NULL** model:

```{r}
#| label: SST Total Sum of Squares
# Calculate overall sum squares SST

SST <- deviance(lm(cmedv ~ 1, data = housing))
SST

```

And here is the SSE:

```{r}
#| label: SSE Within Group Sum of Squares

SSE <- deviance(housing_lm)
SSE

```

Given that the model leaves **unexplained** variations in `cmedv` to the
extent of $SSE$, we can compute the $SSR$, [the Regression Sum of
Squares]{style="background-color: yellow;"}, the amount of variation in
`cmedv` that the linear model **does** explain:

```{r}
#| label: SSR
SSR <- SST - SSE
SSR

```

We have $SST = 42577.74$, $SSE = 21934.39$ and therefore
$SSR = 20643.35$.

In order to calculate the F-Statistic, we need to compute the variances,
using these sum of squares. We obtain variances by dividing by their
*Degrees of Freedom*:

$$
F_{stat} = \frac{SSR / df_{SSR}}{SSE / df_{SSE}}
$$

where $df_{SSR}$ and $df_{SSE}$ are respectively the degrees of freedom
in SSR and SSE.

Let us calculate these Degrees of Freedom. If we have $n=$
`r dim(housing)[[1]]` observations of data, then:

-   $SST$ clearly has degree of freedom
    $n-1 = `r dim(housing)[[1]] -1`$, since it uses all observations but
    loses one degree to calculate the global mean.
-   $SSE$ was computed using the slope and intercept, so it has
    $(n-2) = `r dim(housing)[[1]] -2`$ as degrees of freedom.
-   And therefore $SSR$ being their difference has just $1$ degree of
    freedom.

Now we are ready to compute the F-statistic:

```{r}
n <- housing %>% count() %>% as.numeric()
df_SSR <- 1
df_SSE <- n -2
F_stat <- (SSR/df_SSR) / (SSE/df_SSE)
F_stat

```

The F-stat is compared with a **critical value** of the F-statistic,
which is computed using the formula for the f-distribution in R. As with
our hypothesis tests, we set the significance level to 0.95, and quote
the two relevant degrees of freedom as parameters to `qf()` which
computes the critical F value as a **quartile**:

```{r}
F_crit <-  qf(p = 0.95,     # Significance level is 5%
              df1 = df_SSR, # Numerator degrees of freedom 
              df2 = df_SSE) # Denominator degrees of freedom
F_crit
F_stat

```

The F_crit value can also be seen in a plot[^4]:

```{r}
mosaic::pdist(dist = "f",
              q = F_crit, 
              df1 = df_SSR, df2 = df_SSE)
```

Any value of F more than the $F_{crit}$ occurs with smaller probability
than 0.05. Our F_stat is much higher than $F_{crit}$, by orders of
magnitude! And so we can say with confidence that `rm` has a significant
effect on `cmedv`.

The value of `R.squared` is also calculated from the previously computed
sums of squares:

$$
R.squared = \frac{SSR}{SST} = \frac{SSY-SSE}{SST}
$$ {#eq-rsquared}

```{r}
r_squared <- (SST - SSE)/SST
r_squared
# Also computable by
# mosaic::rsquared(housing_lm)

```

So `R.squared` = `r (SST - SSE)/SST`

The value of Slope and Intercept are computed using a maximum likelihood
derivation and the knowledge that the means square error is a minimum at
the optimum slope: for a linear model $y \sim mx + c$

$$
slope = \frac{\Sigma[(y - y_{mean})*(x - x_{mean})]}{\Sigma(x - x_{mean})^2}
$$

::: callout-tip
Note that the slope is equal to the ratio of the covariance of x and y
to the variance of x.
:::

and

$$
Intercept = y_{mean} - slope * x_{mean}
$$

```{r}
#| label: slope and intercept
slope <- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~ rm, data = housing)
slope

intercept <- mosaic::mean(~ cmedv, data = housing) - slope * mosaic::mean(~ rm, data = housing)
intercept

```

So, there we are! All of this is done for us by one simple formula,
`lm()`!

### {{< iconify iconoir stats-report >}} Using Other Packages {#sec--using-other-packages}

There is a very neat package called `ggstatsplot`[^5] that allows us to
plot very comprehensive statistical graphs. Let us quickly do this:

```{r}
#| message: false
library(ggstatsplot)
housing_lm %>%
  ggstatsplot::ggcoefstats(title = "Linear Model for Boston Housing")


```

This chart shows the estimates for the `intercept` and `rm` along with
their error bars, the `t-statistic`, degrees of freedom, and the
`p-value`.

We can also obtain crisp-looking model tables from the new `supernova`
package [^6], which is based on the methods discussed in Judd et al.

```{r}
#| layout-ncol: 2
library(supernova)
supernova::supernova(housing_lm)

```

This table is very neat in that it gives the `Sums of Squares` for both
the NULL and the current model, for comparison. The `PRE` entry is the
*Proportional Reduction in Error*, a measure that is identical with
`r.squared`, which shows how much the model reduces the error compared
to the NULL model(48%). The PRE idea is nicely discussed in Judd et al.
@sec-references
:::

## {{< iconify flat-color-icons workflow >}} Workflow: Model Checking and Diagnostics {#sec-diagnostics}

We will follow much of the treatment on Linear Model diagnostics, given
[here on the STHDA
website](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#homogeneity-of-variance).

> A first step of this regression diagnostic is to inspect the
> significance of the regression beta coefficients, as well as, the
> R.square that tells us how well the linear regression model fits to
> the data.
>
> For example, the linear regression model makes the assumption that the
> relationship between the predictors (x) and the outcome variable is
> linear. This might not be true. The relationship could be polynomial
> or logarithmic.
>
> Additionally, the data might contain some influential observations,
> such as outliers (or extreme values), that can affect the result of
> the regression.
>
> Therefore, the regression model must be closely diagnosed in order to
> detect potential problems and to check whether the assumptions made by
> the linear regression model are met or not. To do so, we generally
> examine the distribution of **residuals errors**, that can tell us
> more about our data.

### {{< iconify ic twotone-rule >}} Checks for Uncertainty

Let us first look at the uncertainties in the estimates of slope and
intercept. These are most easily read off from the `broom::tidy`-ed
model:

```{r}
#| label: uncertainty and confidence
# housing_lm_tidy <-  housing_lm %>% broom::tidy()
housing_lm_tidy

```

Plotting this is simple too:

```{r}
housing_lm_tidy %>%
  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %>% 
  gf_hline(yintercept = 0) %>% 
  gf_errorbar(conf.low + conf.high ~ term, 
              width = 0.1, 
              title = "Model Estimates with Confidence Intervals") %>% 
  gf_theme(my_theme())

```

### {{< iconify ic twotone-rule >}} Checks for Constant Variance/Heteroscedasticity

Linear Modelling makes 4 fundamental assumptions:("**LINE**")

1.  **Linear** relationship between y and x
2.  Observations are **independent**.
3.  Residuals are **normally** distributed
4.  Variance of the `y` variable is **equal** at all values of `x`.

We can check these using checks and graphs: Here we plot the residuals
against the independent/feature variable and see if there is a gross
variation in their range

```{r}
#| label: Looking at Residuals
#| layout-ncol: 2

housing_lm_augment %>% 
  gf_point(.resid ~ .fitted, title = "Residuals vs Fitted") %>%
  gf_smooth(method = "loess") %>% 
  gf_theme(my_theme())

housing_lm_augment %>% 
  gf_hline(yintercept = 0, colour = "grey", linewidth = 2) %>%
  gf_point(.resid ~ cmedv, title = "Residuals vs Target Variable") %>% 
  gf_theme(my_theme())

housing_lm_augment %>% 
  gf_dhistogram(~ .resid, title = "Histogram of Residuals") %>% 
  gf_fitdistr()%>% 
  gf_theme(my_theme())

housing_lm_augment %>% 
  gf_qq(~ .resid, title = "Q-Q Residuals") %>% 
  gf_qqline() %>% 
  gf_theme(my_theme())

```

The Q-Q plot of residuals also has significant deviations from the
normal quartiles. The residuals are not quite "like the night sky", i.e.
random enough. These point to the need for a richer model, with more
predictors. The "trend line" of residuals vs predictors show a U-shaped
pattern, indicating significant nonlinearity: there is a curved
relationship in the graph. The solution can be a **nonlinear
transformation** of the predictor variables, such as $\sqrt(X)$,
$log(X)$, or even $X^2$. For instance, we might try a model for `cmedv`
using $rm^2$ instead of just `rm` as we have done. This will **still**
be a linear model!

::: callout-tip
Base R has a crisp command to plot these diagnostic graphs. But we will
continue to use `ggformula`.

```{r}
#| eval: false
plot(housing_lm)

```
:::

::: callout-tip
One of the [ggplot extension
packages](https://exts.ggplot2.tidyverse.org/gallery/) named `lindia`
also has a crisp command to plot these diagnostic graphs.

```{r}
#| eval: false
library(lindia)
gg_diagnose(housing_lm)

```
:::

::: column-margin
The `r-squared` for a model `lm(cmedv ~ rm^2)` shows some improvement:

```{r}
#| echo: false
lm(cmedv ~ poly(rm,2), data = housing) %>% 
  broom::glance() %>% select(r.squared) %>% as.numeric()

```
:::

## {{< iconify fluent-mdl2 decision-solid >}} Conclusions

We have seen how starting from a basic EDA of the data, we have been
able to choose a single Quantitative predictor variable to model a
Quantitative target variable, using Linear Regression. As stated
earlier, we may have wish to use more than one predictor variables, to
build more sophisticated models with improved prediction capability. And
there is more than one way of selecting these predictor variables, which
we will examine in the Tutorials in @sec-tutorials.

Secondly, sometimes it may be necessary to mathematically transform the
variables in the dataset to enable the construction of better models,
something that was not needed here.

We may also encounter cases where the predictor variables seem to work
together; one predictor may influence "how well" another predictor
works, something called an *interaction effect* or a *synergy effect*.
We might then have to modify our formula to include *interaction terms*
that look like $predictor1 \times predictor2$.

So our Linear Modelling workflow might look like this: we have not seen
all stages yet, but that is for another course module or tutorial!

```{mermaid}
%%| echo: false
flowchart TD
    A[(A: Data)] -->|mosaic  +  ggformula|B[B:EDA] 
    B --> |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)
    C --> D[D: Decide on Simple/Complex Model]
    D --> E{E: Is the Model Possible?}
    E --> |Yes| G[G: Build Model]
    E -->|Nope| F[F: Transform Variables]
    E -->|Nope| K[K: Try Multiple Regression <br> and/or Interaction Terms]
    K --> D
    F --> D
    G --> H{H: Check Model Diagnostics}
    H --> |Problems| D
    H --> |All   good| I(Interpret Your Model)
    I --> J(((Apply the Model for Predictions)))
    
```

## {{< iconify ooui references-rtl >}} References {#sec-references}

1.  <https://mlu-explain.github.io/linear-regression/>

2.  The Boston Housing Dataset, corrected version. StatLib \@ CMU,
    [lib.stat.cmu.edu/datasets/boston_corrected.txt](http://lib.stat.cmu.edu/datasets/boston_corrected.txt)

3.  <https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R>

4.  Michael Crawley, The R Book,second edition, 2013. Chapter 11.

5.  Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani,
    *Introduction to Statistical Learning*, Springer, 2021. Chapter 3.
    <https://www.statlearning.com/>

6.  David C Howell, [Permutation Tests for Factorial ANOVA
    Designs](https://www.uvm.edu/~statdhtx/StatPages/Permutation%20Anova/PermTestsAnova.html)

7.  Marti Anderson, [Permutation tests for univariate or multivariate
    analysis of variance and
    regression](https://www.academia.edu/50056272/Permutation_tests_for_univariate_or_multivariate_analysis_of_variance_and_regression?auto=download)

8.  Schloerke B, Cook D, Larmarange J, Briatte F, Marbach M, Thoen E,
    Elberg A, Crowley J (2022). *GGally: Extension to 'ggplot2'*.
    <https://ggobi.github.io/ggally/>,
    <https://github.com/ggobi/ggally>.

9.  <http://r-statistics.co/Assumptions-of-Linear-Regression.html>

10. Judd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017.
    "Introduction to Data Analysis." In, 1--9. Routledge.
    <https://doi.org/10.4324/9781315744131-1>. Also see
    <http://www.dataanalysisbook.com/index.html>

11. Patil, I. (2021). Visualizations with statistical details: The
    'ggstatsplot' approach. Journal of Open Source Software, 6(61),
    3167,
    [https://doi:10.21105/joss.03167](https://doi:10.21105/joss.03167){.uri}

[^1]: The model is linear in the **parameters** $\beta_i$, e.g. We can
    have this: $$
    y_i \sim \beta_1*x_i + \beta_0\\
    $$ or $$
    y_1 \sim exp(\beta_1)*x_i + \beta_0
    $$ but not: $$
    y_i \sim \beta_1*exp(\beta_2*x_i) + \beta_0\\
    $$ or $$
    y_i \sim \beta_1 *x^{\beta_2} + \beta_0
    $$

[^2]: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani,
    *Introduction to Statistical Learning, Springer, 2021. Chapter 3.
    Linear Regression*. [Available
    Online](https://www.statlearning.com/)

[^3]: Michael Crawley, *The R Book, Third Edition 2023. Chapter 9.
    Statistical Modelling*

[^4]: Michael Crawley, *The R Book, Third Edition 2023. Chapter 9.
    Statistical Modelling*

[^5]: <https://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html>

[^6]: <https://github.com/UCLATALL/supernova>
