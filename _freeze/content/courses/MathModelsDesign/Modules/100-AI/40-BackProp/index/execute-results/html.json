{
  "hash": "bd768f07a5daac4987ac196036da6dca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 23/Nov/2024\ndate-modified: \"2025-05-17\"\ntitle: \"MLPs and Backpropagation\"\norder: 40\nsummary: \ntags:\n- Neural Nets\n- Back Propagation\n- Gradient\nfilters:\n  - d2\nd2:\n  layout: elk\n  theme: \"CoolClassics\"\n  sketch: true\n\n---\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n### How does an MLP Learn?\n\nWe saw how each layer works:\n\n$$\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} = \nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} * \n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n$$  {#eq-fwd-prop-1}\n\n\n\nand:\n\n$$\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n$$ {#eq-fwd-prop-2}\n\n\n\nSee how the connections between neurons are marked by **weights**: these multiply the signal from the previous neuron. The multiplied/weighted products are added up in the neuron, and the sum is given to the activation block therein. \n\nSo learning?\n\nThe only controllable variables in a neural network are these weights! So ***learning involves adapting these weights so that they can perform a useful function***. \n\n## What is the Learning Process?\n\nThe process of *adapting the weights* of a neural network can be described in the following steps:\n\n- **Training Set**: Training is over several known input-output pairs (\"training data\")\n- **Training Epoch**: For each input, the signals propagate forward until we have an output\n- **Error Calculation**: Output is compared with **desired output**, to calculate *error*\n- **Backpropagation**: Each neuron (and its weights) need to be told what is their share of the error! Errors therefore need to be *sent backward from the output* to input, unravelling them from layer $l$ to layer $l-1$. (like apportioning blame !!). \n- **Error-to-Cost**: How does error at *any* given neuron relate to the idea of an **overall Cost** function? Is the Cost function also **apportioned** in the same way?\n- **Differentiate**: Evaluate the *effect* of each weight/bias on ~~the (apportioned) error~~ overall Cost. (Slope!!)\n- **Gradient Descent**: Adapt the weights/biases with a small step in the **opposite direction** to the slope.\n\nThere.\n\n### What is the Output Error?\n\nIf $d(k)$ are the desired outputs of the NN (over an entire training set), and $y(k)$ are the outputs of the ***output layer***, then we calculate the error **at the outputs** of the NN as:\n\n$$\ne(k) = a(k) - d(k)\n$${#eq-error-function}\n\nThis error is calculated at *each output* for each training epoch/sample/batch. (More about the batch-mode in a bit.)\n\n### What is the Cost Function?\n\nWe define the **cost** or **objective** function as the *squared error averaged over all neurons*:\n\n$$\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\\\\n&= \\frac{1}{2n}\\sum^{n~neurons}_{k=1}(a_i - d_i)^2\n\\end{align}\n$${#eq-cost-function}\n\n\nThe $a_i$s are the outputs of $n$ neurons and $d_i$ are the desired outputs for each of the training samples.\n\n[The Cost Function is of course dependent upon the *Weights* and the *biases*]{.bg-light-red .black }, and is to be minimized by adapting these. Using the sum of *squared errors*, along with the *linear* operations in the NN guarantees that the Cost Function (usually) has one global, minimum. \n\n::: {.cell warn='false'}\n\n:::\n\n### What is Backpropagation of Error?\n\nAs we stated earlier, error is calculated at the output. In order to adapt **all weights**, we need to *send error proportionately back along the network*, towards the input. This proportional error will give us a basis to adapt the individual weights anywhere in the network. \n\nWhat does \"proportional\" mean here? Consider the diagram below:\n\n\n::: {.cell}\n\n```{.d2 .cell-code}\ntitle: Error Backpropagated from e12 {\n  shape: text\n  near: top-center\n  style: {\n    font-size: 60\n    italic: true\n  }\n}\ndirection: right\ngrid-columns: 3\ngrid-gap: 400\n\nlayer-1: {\n  grid-columns: 1\n  grid-gap: 100\n  1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\nlayer-2: {\n  grid-columns: 1\n  grid-gap: 100\n  h1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\n\nlayer-3: {\n  grid-columns: 1\n  grid-gap: 100\n  style: {\n    opacity: 0\n  }\n    e12: \"e12\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    e22: \"e22\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    e32: \"e32\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n     }\n     }\n}\nlayer-1.1 <-> layer-2.h1: W11 {\n  source-arrowhead: e11\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.1 -> layer-2.h2\nlayer-1.1 -> layer-2.h3\nlayer-1.2 <-> layer-2.h1: W21 {\n  source-arrowhead.label: e21\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.2 -> layer-2.h2\nlayer-1.2 -> layer-2.h3\nlayer-1.3 <-> layer-2.h1: W31 {\n  source-arrowhead.label: e31\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.3 -> layer-2.h2\nlayer-1.3 -> layer-2.h3\n\nlayer-2.h1 <-> layer-3.e12 { style: {stroke-width: 9\n         stroke: FireBrick}}\nlayer-2.h2 -> layer-3.e22\nlayer-2.h3 -> layer-3.e32\n\n```\n:::\n\n:::: {.columns}\n::: {.column width=48%}\n\n$$\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}}\\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{21}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}} \\\\\ne_{31}~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{magenta}{\\pmb{h_1}}} \\\\\n\\end{align}\n$$\n:::\n::: {.column width=4%}\n:::\n::: {.column width=48%}\n\n$$\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{21}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{31} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{31}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}}  \\\\\n\\end{align}\n$$\n:::\n::::\n\nThese are the contributions of the error $e_{12}$ to each of the previous neurons. \n\nAnother way of looking at this:\n\n\n::: {.cell}\n\n```{.d2 .cell-code}\ntitle: Total Error at e11 {\n  shape: text\n  near: top-center\n  style: {\n    font-size: 60\n    italic: true\n  }\n}\ndirection: right\ngrid-columns: 3\ngrid-gap: 400\n\nlayer-1: {\n  grid-columns: 1\n  grid-gap: 100\n  1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\nlayer-2: {\n  grid-columns: 1\n  grid-gap: 100\n  h1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\n\nlayer-3: {\n  grid-columns: 1\n  grid-gap: 100\n  style: {\n    opacity: 0\n  }\n    e12: \"e12\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    e22: \"e22\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    e32: \"e32\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n     }\n     }\n}\nlayer-1.1 <-> layer-2.h1: W11 {\n  source-arrowhead.label: e11\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.1 <-> layer-2.h2: W12 {\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.1 <-> layer-2.h3: W13 {\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.2 -> layer-2.h1\nlayer-1.2 -> layer-2.h2{\n  source-arrowhead.label: e21\n  style: {\n    font-size: 45\n  }\n}\nlayer-1.2 -> layer-2.h3\nlayer-1.3 -> layer-2.h1{\n  source-arrowhead.label: e31\n  style: {\n    font-size: 45\n  }\n}\nlayer-1.3 -> layer-2.h2\nlayer-1.3 -> layer-2.h3\n\nlayer-2.h1 -> layer-3.e12 { style: {stroke-width:9 \n         stroke: FireBrick}}\nlayer-2.h2 ->layer-3.e22  { style: {stroke-width:9\n         stroke: FireBrick}}\nlayer-2.h3 ->layer-3.e32  { style: {stroke-width:9\n         stroke: FireBrick}}\n\n```\n:::\n\n:::: {.columns}\n::: {.column width=48%}\n$$\n\\begin{align}\ne_{11} =~ e_{12} * \\frac{W_{11}}{Sum~of~weights~to~{\\color{orange}{\\pmb {h_1}}}}\\\\\n+ ~ e_{22} * \\frac{W_{21}}{Sum~of~Weights~to~\\color{pink}{\\pmb{h_2}}} \\\\\n+ ~ e_{32} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{teal}{\\pmb{h_3}}}  \\\\\n\\end{align}\n$$\n:::\n::: {.column width=4%}\n:::\n::: {.column width=48%}\n$$\n\\begin{align}\ne_{11} = ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{orange}{W_{11} + W_{21} + W_{31}}}}\\\\\n+ ~e_{22} * \\frac{W_{12}}{\\pmb{\\color{pink}{W_{12} + W_{22} + W_{32}}}} \\\\\n+ ~e_{32} * \\frac{W_{13}}{\\pmb{\\color{teal}{W_{13} + W_{23} + W_{33}}}}  \\\\\n\\end{align}\n$$\n\n\n$$\n\\begin{align}\ne_{21} = similar~expression!!\\\\\n\\\ne_{31} = similar~expression!!\\\\\n\\end{align}\n$$\n:::\n::::\n\n[Equation corrected by Gayatri Jadhav, April 2025]{.aside}\n\nThis is the ***total error*** at $e_{11}$ from all the three output errors. So:\n\n- We have taken each output error, $e_{*2}$ and parcelled it back to the preceding neurons ***in proportion to the connecting Weight***. This makes intuitive sense; we are making those neurons put their money where their mouth is. As [Nassim Nicholas Taleb](https://philosophiatopics.wordpress.com/wp-content/uploads/2018/10/skin-in-the-game-nassim-nicholas-taleb.pdf) says, people (and neurons!) need to pay for their opinions, especially when things go wrong!\n- The *accumulated error* at each neuron in layer $l-1$ is the weighted sum of back-propagated error contributions from all layer $l$ neurons to which we are connected. \n- So we can compactly write the relationships above as:\n\n$$\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} = \n\\Bigg(\n\\begin{bmatrix}\n\\frac{W_{11}}{D_{11}} & \\frac{W_{12}}{D_{12}} & \\frac{W_{13}}{D_{13}}\\\\\n\\frac{W_{21}}{D_{21}} & \\frac{W_{22}}{D_{22}} & \\frac{W_{23}}{D_{23}}\\\\\n\\frac{W_{31}}{D_{31}} & \\frac{W_{32}}{D_{32}} & \\frac{W_{33}}{D_{33}}\\\\\n\\end{bmatrix} * \n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\Bigg)\n$$\n\nThe denominators make things look complicated! But if we are able to [simply ignore them]{.black .bg-light-red} for a moment, then we see a very interesting thing:\n\n$$\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} * \n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n$$\n\n[This new *approximate* matrix is the **tranpose** of our original Weight matrix]{.black .bg-light-red} from @eq-fwd-prop-1! The rows there have become columns here!! That makes intuitive sense: in the forward information direction, we were accounting for information from the point of view of the ***destinations***; in the reverse error backpropagation direction, we are accounting for information from the point of view of the ***sources***.\n\nWriting this equation in a compact way:\n\n$$\n\\Large{e^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}}\n$${#eq-Back-Prop}\n\nThis is our equation for backpropagation of error. \n\nWhy is ignoring all those *individual* denominators justified? Let us park that question until we have understood the one last step in NN training, the [Gradient Descent.](../50-GradientDescent/index.qmd)\n\n\n::: {.content-hidden}\n## Backpropagation Numerically Demonstrated\n\n::: {.cell}\n\n```{.d2 .cell-code}\ndirection: right\ngrid-columns: 6\ngrid-rows: 3\n###\nin1.style.opacity: 0\nin2.style.opacity: 0\nin3.style.opacity: 0\n1.shape: circle\n2.shape: circle\n3.shape: circle\nh1.shape: circle\nh2.shape: circle\nh3.shape: circle\no1.shape: circle\n# o1 {\n#   icon: https://icons.terrastruct.com/infra/019-network.svg\n# }\no2.shape: circle\no3.shape: circle\nout1.style.opacity: 0\nout2.style.opacity: 0\nout3.style.opacity: 0\n###\nin1 -> 1\nin2 -> 2\nin3 -> 3\n1 -> h1: w21 {\n  style: {\n    stroke: deepskyblue}\n}\n1 -> h2: W21 {\n  style: {\n  fill: LightBlue\n  stroke: FireBrick\n  stroke-width: 2\n  animated: true\n  }\n}\n1 -> h3\n2 -> h1\n2 -> h2\n2 -> h3\n3 -> h1\n3 -> h2\n3 -> h3\nh1 -> o1\nh2 -> o1\nh3 -> o1\nh1 -> o2\nh2 -> o2\nh3 -> o2\nh1 -> o3\nh2 -> o3\nh3 -> o3\n\no1 -> out1\no2 -> out2\no3 -> out3\n\n```\n:::\n:::\n\n\n## Here Comes the ~~Rain~~ Maths Again!\n\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.ka. 3Blue1Brown.\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n{{< video https://youtu.be/QJoa0JYaX1I?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n:::\n::::\n\n## Backpropagation in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke \"}\n\n### Using p5.js\n\n\n### Using R\nUsing `torch`.\n\n:::\n\n\n## References\n\n1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)\n1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>\n1. Interactive Backpropagation Explainer <https://xnought.github.io/backprop-explainer/>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}