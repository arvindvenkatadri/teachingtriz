{
  "hash": "459ca9059494704289d70daea64753f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Perceptron\"\ndate-modified: \"2025-04-27\"\ndate: 20/Nov/2024\norder: 20\nsummary: \ntags:\n- Neural Nets\n- Perceptron\n- Weighted Average\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## {{< iconify icons8 idea >}} Inspiration\n\n## What is a Perceptron?\n\nThe [perceptron](../../../../../materials/pdfs/Rosenblatt1958.pdf) was\ninvented by Frank Rosenblatt is considered one of the foundational\npieces of neural network structures. The output is viewed as a\n*decision* from the neuron and is usually propagated as an input to\nother neurons inside the neural network.\n\n![Perceptron](../../../../../materials/images/png/Neurons.png){height=\"360\"}\n\n\n#### Math Intuition\n\n-   We can imagine this as a set of inputs that *averaged in weighted\n    fashion*.\n\n$$\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n$${#eq-perceptron}\n\n-   Since the inputs are added with linear weighting, this effectively\n    acts like a **linear transformation** of the input data.\n    -   A linear equation of this sort is the [general equation of an\n        n-dimensional plane](https://www.geeksforgeeks.org/equation-of-plane/).\n    -   If we imagine the input as representing the n-coordinates in a\n        plane, then the multiplications scale/stretch/compress the\n        plane, like a rubber sheet. (But do not **fold it**.)\n    -   If there were only 2 inputs, we could mentally picture this with a handkerchief.\n-   More metaphorically, it seems like the neuron is consulting each of\n    the inputs, asking for their opinion, and then making a decision by\n    attaching *different amounts of significance* to each opinion.\n-   The Structure should remind you of [Linear\n    Regression](../../../../Analytics/Modelling/Modules/LinReg/index.qmd)\n    !\n\nSo how does it work? Consider the interactive diagram below:\n\n<iframe height=480 width=680\nsrc=\"https://editor.p5js.org/arvindv/full/qx7rMp3RI\">\n</iframe>\n\n- The coordinate axes are as shown [X]{.red}, [Y]{.green}, and [Z]{.blue} \n\n- The [grey]{.black .bg-gray} and [yellow]{.black .bg-yellow} points are the data we wish to classify into two categories, unsurprisingly \"yellow\" and grey\".\n- The [Weight vector]{.black .bg-red} line is a vector of all the weights in the Perceptron. \n\n- Now, as per the [point-normal form of an n-dimensional plane](https://www.geeksforgeeks.org/equation-of-plane/), the multiplication of the input data with the weight vector is like taking a ***vector dot product*** ( aka inner product) ! And: every point on the plane has a dot product of ZERO. See the [purple vector]{.bg-purple} which is **normal** to the Weight vector: its dot product with the Weight vector is zero. \n- Data points that are off this \"normal plane\" in either direction (above and below) will have dot-products which will be positive or negative depending upon the direction!\n- Hence we can use the dot-product POLARITY to decide if a point is above or below the plane defined by the Weight vector. Which is what is done in the threshold-based activation!\n- The bias $b$ defines the POSITION of the plane; and the Weights define the direction. Together, they classify the points based on the @eq-perceptron. \n- Try to move the slider to get an intuition of how the plane moves with the bias.  Clearly, the bias is very influential in deciding the POLARITY of the dot-products. When it aligns with the purple vector ($dot product = 0$), it works best. \n\n\n#### Why \"Linear\"?\n\nWhy are (almost) all operations linear operations in a NN?\n\n-   We said that the weighted sums *are* a linear operation, but why is\n    this so?\n-   We wish to be able to set-up analytic functions for performance of\n    the NN, and **be able to differentiate them** to be able to optimize\n    them.\n-   Non-linear blocks, such as *threshold blocks/signum-function based\n    slicers* are not differentiable and we are unable to set up such\n    analysis.\n-   Note the title of [this\n    reference](https://www.sscardapane.it/alice-book/).\n\n#### Why is there a Bias input?\n\n-   We want the weighted sum of the inputs to *mean something*\n    significant, before we accept it.\n-   The bias is *subtracted* from the weighted sum of inputs, and the\n    bias input could also (notionally) have a weight.\n-   The *bias* is like a threshold which the weighted sum has to exceed;\n    if it does, the neuron is said to **fire**.\n\n\nSo with all that vocabulary, we *might* want to watch this longish video\nby the great Dan Shiffman:\n\n{{< video https://youtu.be/ntKn5TPHHAk?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}\n\n## Perceptrons in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke;\"}\n### {{< iconify la r-project >}} R\n\nLet us try a simple single layer NN in R. We will use the R package\n`neuralnet`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load the package\n# library(neuralnet)\n\n# Use iris\n# Create Training and Testing Datasets\ndf_train <- iris %>% slice_sample(n = 100)\ndf_test <- iris %>% anti_join(df_train)\nhead(iris)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Sepal.Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sepal.Width\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Length\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Width\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species\"],\"name\":[5],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"5.1\",\"2\":\"3.5\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"1\"},{\"1\":\"4.9\",\"2\":\"3.0\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"2\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"3\"},{\"1\":\"4.6\",\"2\":\"3.1\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"4\"},{\"1\":\"5.0\",\"2\":\"3.6\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"5\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"3\":\"1.7\",\"4\":\"0.4\",\"5\":\"setosa\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a simle Neural Net\nnn <- neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = df_train,\n  hidden = 0,\n  # act.fct = \"logistic\", # Sigmoid\n  linear.output = TRUE\n) # TRUE to ignore activation function\n\n# str(nn)\n\n# Plot\nplot(nn)\n\n# Predictions\n# Predict <- compute(nn, df_test)\n# Predict\n# cat(\"Predicted values:\\n\")\n# print(Predict$net.result)\n#\n# probability <- Predict$net.result\n# pred <- ifelse(probability > 0.5, 1, 0)\n# cat(\"Result in binary values:\\n\")\n# pred %>% as_tibble()\n```\n:::\n\n\n![](nn.png)\n\n### {{< iconify skill-icons p5js>}} p5.js\n\nTo Be Written Up.\n:::\n\n## References\n\n1.  The Neural Network Zoo - The Asimov Institute.\n    <http://www.asimovinstitute.org/neural-network-zoo/>\n2.  It’s just a linear model: neural networks edition.\n    <https://lucy.shinyapps.io/neural-net-linear/>\n3.  Neural Network Playground. <https://playground.tensorflow.org/>\n4.  Rohit Patel (20 Oct 2024). *Understanding LLMs from Scratch Using\n    Middle School Math: A self-contained, full explanation to inner\n    workings of an LLM*.\n    <https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876>\n5.  Machine Learning Tokyo: Interactive Tools for ML/DL, and Math.\n    <https://github.com/Machine-Learning-Tokyo/Interactive_Tool>\n6.  *Anyone Can Learn AI Using This Blog*.\n    <https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl>\n7.  Neural Networks Visual with\n    [vcubingx](https://youtube.com/@vcubingx?feature=shared)\n    -   Part 1. <https://youtu.be/UOvPeC8WOt8>\n    -   Part 2. <https://www.youtube.com/watch?v=-at7SLoVK_I>\n8.  Practical Deep Learning for Coders: An Online Free\n    Course.<https://course.fast.ai>\n\n#### Text Books\n\n1.  Michael Nielsen. *Neural Networks and Deep Learning*, a free online\n    book. <http://neuralnetworksanddeeplearning.com/index.html>\n2.  Simone Scardapane. (2024) *Alice’s Adventures in a differentiable\n    Wonderland*.<https://www.sscardapane.it/alice-book/>\n\n#### Using R for DL\n\n1.  David Selby (9 January 2018). Tea and Stats Blog. *Building a neural\n    network from scratch in R*.\n    <https://selbydavid.com/2018/01/09/neural-network/>\n2.  *torch for R: An open source machine learning framework based on\n    PyTorch.* <https://torch.mlverse.org>\n3.  Akshaj Verma. (2020-07-24). *Building A Neural Net from Scratch\n    Using R - Part 1 and Part 2*.\n    <https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/>\n    and\n    <https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/>\n\n#### Maths\n\n1.  Parr and Howard (2018). *The Matrix Calculus You Need for Deep\n    Learning*.<https://arxiv.org/abs/1802.01528>\n\n::: {#refs style=\"font-size: 60%;\"}\n###### {{< iconify lucide package-check >}} R Package Citations\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nPackage     Version   Citation   \n----------  --------  -----------\nneuralnet   1.44.2    @neuralnet \n\n\n:::\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}