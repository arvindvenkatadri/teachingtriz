{
  "hash": "2a722a85bdcb633972ed8763f68d2099",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Perceptron\"\ndate-modified: \"2025-04-09\"\ndate: 20/Nov/2024\norder: 20\nsummary: \ntags:\n- Neural Nets\n- Perceptron\n- Weighted Average\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n## {{< iconify icons8 idea >}} Inspiration\n\n## What is a Perceptron?\n\nThe [perceptron](../../../../../materials/pdfs/Rosenblatt1958.pdf) was\ninvented by Frank Rosenblatt is considered one of the foundational\npieces of neural network structures. The output is viewed as a\n*decision* from the neuron and is usually propagated as an input to\nother neurons inside the neural network.\n\n![Perceptron](../../../../../materials/images/png/Neurons.png){height=\"360\"}\n\n#### Math Intuition\n\n-   We can imagine this as a set of inputs that *averaged in weighted\n    fashion*.\n\n$$\ny_k = sigmoid~(~\\sum_{k=1}^n W_k*x_k + b~)\n$$\n\n-   Since the inputs are added with linear weighting, this effectively\n    acts like a **linear transformation** of the input data.\n    -   A linear equation of this sort is the [equation of an\n        n-dimensional\n        plane](https://www.geeksforgeeks.org/equation-of-plane/).\n    -   If we imagine the input as representing the n-coordinates in a\n        plane, then the multiplications scale/stretch/compress the\n        plane, like a rubber sheet. (But do not **fold it**.)\n    -   If there were only 2 inputs, we could mentally picture this.\n-   More metaphorically, it seems like the neuron is consulting each of\n    the inputs, asking for their opinion, and then making a decision by\n    attaching *different amounts of significance* to each opinion.\n-   The Structure should remind you of [Linear\n    Regression](../../../../Analytics/Modelling/Modules/LinReg/index.qmd)\n    !\n\n#### Why \"Linear\"?\n\nWhy are (almost) all operations linear operations in a NN?\n\n-   We said that the weighted sums *are* a linear operation, but why is\n    this so?\n-   We wish to be able to set-up analytic functions for performance of\n    the NN, and **be able to differentiate them** to be able to optimize\n    them.\n-   Non-linear blocks, such as *threshold blocks/signum-function based\n    slicers* are not differentiable and we are unable to set up such\n    analysis.\n-   Note the title of [this\n    reference](https://www.sscardapane.it/alice-book/).\n\n#### Why is there a Bias input?\n\n-   We want the weighted sum of the inputs to *mean something*\n    significant, before we accept it.\n-   The bias is *subtracted* from the weighted sum of inputs, and the\n    bias input could also (notionally) have a weight.\n-   The *bias* is like a threshold which the weighted sum has to exceed;\n    if it does, the neuron is said to **fire**.\n\n#### What is the Activation Block?\n\n-   We said earlier that the weighting and adding is a linear operation.\n-   While this is great, simple linear translations of data are not\n    capable of generating what we might call learning or generalization\n    ability.\n-   We need to have some non-linear block to allow the data to create\n    nonlinear transformations of the data space, such as *curving it, or\n    folding it, or creating bumps, depressions, twists*, and so on.\n\n![Activation](../../../../../materials/images/png/Activation_Functions.png){height=\"360\"}\n\n-   This nonlinear function needs to be chosen with care so that it is\n    both differentiable and keeps the math analysis tractable. (More\n    later)\n-   Such a nonlinear mathematical function is implemented in the\n    **Activation Block**.\n-   See this example: red and blue areas, which we wish to **separate\n    and classify these** with our DLNN, are not separable unless we fold\n    and curve our 2D data space.\n-   The separation is achieved using a linear operation, i.e. a LINE!!\n\n![From [Colah\nBlog](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif),\nused sadly without\npermission](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif){#fig-manifolds-and-neural-networks}\n\n-   For instance in @fig-manifolds-and-neural-networks, no amount of\n    stretching or compressing of the surface can separate the two sets (\n    blue and red ) using a line or plane, **unless** the surface can be\n    warped into **another dimension** by folding.\n\n#### What is the Sigmoid Function?\n\nSo how do we implement this nonlinear Activation Block?\n\n-   One of the popular functions used in the Activation Block is a\n    function based on the exponential function $e^x$.\n-   Why? Because this function retains is identity when differentiated!\n    This is a very convenient property!\n\n![Sigmoid\nActivation](../../../../../materials/images/png/Sigmoid.png){height=\"360\"}\n\n::: callout-note\n#### Remembering Logistic Regression\n\nRecall your study of [Logistic\nRegression](../../../../Analytics/Modelling/Modules/LogReg/index.qmd).\nThere, the Sigmoid function was used to model the odds of the\n(Qualitative) target variable against the (Quantitative) predictor.\n:::\n\n::: callout-note\n#### But Why Sigmoid?\n\nBecause [the Sigmoid function is\ndifferentiable](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x).\nAnd linear in the mid ranges. Oh, and remember the [Chain\nRule](https://en.wikipedia.org/wiki/Derivative#Rules_for_basic_functions)?\n\n$$\n\\begin{align}\n\\frac{df(x)}{dx}\n&= \\frac{d}{dx} * \\frac{1}{1 + e^{-x}} \\\\\\\n&= -(1 + e^{-x})^{-2} * \\frac{d}{dx}(1 + e^{-x})~~\\text{(Using Chain Rule)}\\\\\n&= -(1 + e^{-x})^{-2} * (-e^{-x})\\\\\n&=  \\frac{e^{-x}}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{1}{1 + e^{-x}} * \\Bigg({\\frac{1 + e^{-x}}{1 + e^{-x}}} - \\frac{1}{1 + e^{-x}}\\Bigg)\\\\\\\n&\\text{ and therefore}\\\\\\\n\\Large{\\frac{df(x)}{dx}} &= \\Large{f(x) * (1 - f(x))}\\\\\n\\end{align}\n$$\n:::\n\nSo with all that vocabulary, we *might* want to watch this longish video\nby the great Dan Shiffman:\n\n{{< video https://youtu.be/ntKn5TPHHAk?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}\n\n## Perceptrons in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke;\"}\n### {{< iconify la r-project >}} R\n\nLet us try a simple single layer NN in R. We will use the R package\n`neuralnet`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load the package\n# library(neuralnet)\n\n# Use iris\n# Create Training and Testing Datasets\ndf_train <- iris %>% slice_sample(n = 100)\ndf_test <- iris %>% anti_join(df_train)\nhead(iris)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Sepal.Length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sepal.Width\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Length\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Petal.Width\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species\"],\"name\":[5],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"5.1\",\"2\":\"3.5\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"1\"},{\"1\":\"4.9\",\"2\":\"3.0\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"2\"},{\"1\":\"4.7\",\"2\":\"3.2\",\"3\":\"1.3\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"3\"},{\"1\":\"4.6\",\"2\":\"3.1\",\"3\":\"1.5\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"4\"},{\"1\":\"5.0\",\"2\":\"3.6\",\"3\":\"1.4\",\"4\":\"0.2\",\"5\":\"setosa\",\"_rn_\":\"5\"},{\"1\":\"5.4\",\"2\":\"3.9\",\"3\":\"1.7\",\"4\":\"0.4\",\"5\":\"setosa\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a simle Neural Net\nnn <- neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = df_train,\n  hidden = 0,\n  # act.fct = \"logistic\", # Sigmoid\n  linear.output = TRUE\n) # TRUE to ignore activation function\n\n# str(nn)\n\n# Plot\nplot(nn)\n\n# Predictions\n# Predict <- compute(nn, df_test)\n# Predict\n# cat(\"Predicted values:\\n\")\n# print(Predict$net.result)\n#\n# probability <- Predict$net.result\n# pred <- ifelse(probability > 0.5, 1, 0)\n# cat(\"Result in binary values:\\n\")\n# pred %>% as_tibble()\n```\n:::\n\n![](nn.png)\n\n### {{< iconify skill-icons p5js>}} p5.js\n\nTo Be Written Up.\n:::\n\n## References\n\n1.  The Neural Network Zoo - The Asimov Institute.\n    <http://www.asimovinstitute.org/neural-network-zoo/>\n2.  It’s just a linear model: neural networks edition.\n    <https://lucy.shinyapps.io/neural-net-linear/>\n3.  Neural Network Playground. <https://playground.tensorflow.org/>\n4.  Rohit Patel (20 Oct 2024). *Understanding LLMs from Scratch Using\n    Middle School Math: A self-contained, full explanation to inner\n    workings of an LLM*.\n    <https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876>\n5.  Machine Learning Tokyo: Interactive Tools for ML/DL, and Math.\n    <https://github.com/Machine-Learning-Tokyo/Interactive_Tool>\n6.  *Anyone Can Learn AI Using This Blog*.\n    <https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl>\n7.  Neural Networks Visual with\n    [vcubingx](https://youtube.com/@vcubingx?feature=shared)\n    -   Part 1. <https://youtu.be/UOvPeC8WOt8>\n    -   Part 2. <https://www.youtube.com/watch?v=-at7SLoVK_I>\n8.  Practical Deep Learning for Coders: An Online Free\n    Course.<https://course.fast.ai>\n\n#### Text Books\n\n1.  Michael Nielsen. *Neural Networks and Deep Learning*, a free online\n    book. <http://neuralnetworksanddeeplearning.com/index.html>\n2.  Simone Scardapane. (2024) *Alice’s Adventures in a differentiable\n    Wonderland*.<https://www.sscardapane.it/alice-book/>\n\n#### Using R for DL\n\n1.  David Selby (9 January 2018). Tea and Stats Blog. *Building a neural\n    network from scratch in R*.\n    <https://selbydavid.com/2018/01/09/neural-network/>\n2.  *torch for R: An open source machine learning framework based on\n    PyTorch.* <https://torch.mlverse.org>\n3.  Akshaj Verma. (2020-07-24). *Building A Neural Net from Scratch\n    Using R - Part 1 and Part 2*.\n    <https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/>\n    and\n    <https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/>\n\n#### Maths\n\n1.  Parr and Howard (2018). *The Matrix Calculus You Need for Deep\n    Learning*.<https://arxiv.org/abs/1802.01528>\n\n::: {#refs style=\"font-size: 60%;\"}\n###### {{< iconify lucide package-check >}} R Package Citations\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nPackage     Version   Citation   \n----------  --------  -----------\nneuralnet   1.44.2    @neuralnet \n\n\n:::\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}